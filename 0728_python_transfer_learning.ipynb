{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlp09u4RXn/SBv7o8hjNWW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex-Jung-HB/0728_python_transfer-learning/blob/main/0728_python_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Mount a google drive"
      ],
      "metadata": {
        "id": "LfxjF8pQePNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF4m5_eZykW-",
        "outputId": "71ade7d9-98b6-470a-bb0e-3e84dfe50a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File copy to Colab"
      ],
      "metadata": {
        "id": "UEkGfYBbeXo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ZIP ÌååÏùºÏùÑ ÏΩîÎû©ÏúºÎ°ú Î≥µÏÇ¨\n",
        "!cp \"/content/drive/MyDrive/dataset.zip\" \"/content/\"\n",
        "\n",
        "# ÏïïÏ∂ï Ìï¥Ï†ú\n",
        "!unzip -o /content/dataset.zip -d /content/\n",
        "\n",
        "# ÏïïÏ∂ï Ìï¥Ï†ú ÌôïÏù∏\n",
        "!ls -la /content/"
      ],
      "metadata": {
        "id": "xneYgsP4z-XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "File copy to Colab"
      ],
      "metadata": {
        "id": "FuA9pjYYdQCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìö COMPLETE ANNOTATED GUIDE - ZIP FILE IN GOOGLE COLAB\n",
        "# Every command explained step by step for easy understanding\n",
        "\n",
        "# =============================================================================\n",
        "# üîó STEP 1: MOUNT GOOGLE DRIVE\n",
        "# =============================================================================\n",
        "\n",
        "# Import the drive module from google.colab\n",
        "# This module allows us to connect Colab to your Google Drive account\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount (connect) your Google Drive to Colab\n",
        "# This creates a bridge between Colab and your personal Google Drive\n",
        "# After this, your Drive files will be accessible at /content/drive/MyDrive/\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# üìù What happens when you run this:\n",
        "# - A popup appears asking for permission to access your Google Drive\n",
        "# - You click \"Connect to Google Drive\"\n",
        "# - You sign in to your Google account\n",
        "# - You grant permission to Colab\n",
        "# - Your Drive becomes accessible in Colab at the path /content/drive/MyDrive/\n",
        "\n",
        "# =============================================================================\n",
        "# üîç STEP 2: VERIFY THE FILE EXISTS (OPTIONAL BUT RECOMMENDED)\n",
        "# =============================================================================\n",
        "\n",
        "# List detailed information about your specific ZIP file\n",
        "# -l = long format (shows permissions, size, date, owner)\n",
        "# -a = show all files (including hidden ones starting with .)\n",
        "!ls -la \"/content/drive/MyDrive/dataset.zip\"\n",
        "\n",
        "# üìù What this command does:\n",
        "# - Checks if dataset.zip actually exists in your Google Drive\n",
        "# - Shows file size, creation date, and permissions\n",
        "# - If file doesn't exist, you'll get \"No such file or directory\" error\n",
        "# - If file exists, you'll see something like: -rw------- 1 root root 140123456 Jul 28 dataset.zip\n",
        "\n",
        "# Alternative: Check what's in your entire Drive root folder\n",
        "!ls -la /content/drive/MyDrive/\n",
        "\n",
        "# üìù This shows ALL files and folders in your Google Drive root\n",
        "# Helpful to see everything you have and find your ZIP file\n",
        "\n",
        "# =============================================================================\n",
        "# üìÇ STEP 3: COPY FILE FROM GOOGLE DRIVE TO COLAB WORKSPACE\n",
        "# =============================================================================\n",
        "\n",
        "# Copy the ZIP file from Google Drive to Colab's local storage\n",
        "# cp = copy command\n",
        "# \"/content/drive/MyDrive/dataset.zip\" = source (where to copy FROM)\n",
        "# /content/ = destination (where to copy TO)\n",
        "!cp \"/content/drive/MyDrive/dataset.zip\" /content/\n",
        "\n",
        "# üìù Why we use quotes around the source path:\n",
        "# - Protects against special characters or spaces in filenames\n",
        "# - Good practice even when not strictly necessary\n",
        "# - \"/content/drive/MyDrive/dataset.zip\" is safer than /content/drive/MyDrive/dataset.zip\n",
        "\n",
        "# üìù Understanding the paths:\n",
        "# - /content/drive/MyDrive/ = Your Google Drive (permanent storage)\n",
        "# - /content/ = Colab's workspace (temporary storage, deleted when session ends)\n",
        "# - We copy from permanent storage to temporary workspace for faster processing\n",
        "\n",
        "# =============================================================================\n",
        "# ‚úÖ STEP 4: VERIFY THE COPY WAS SUCCESSFUL\n",
        "# =============================================================================\n",
        "\n",
        "# Check if the file was successfully copied to Colab's workspace\n",
        "!ls -la /content/dataset.zip\n",
        "\n",
        "# üìù What to expect:\n",
        "# - If successful: Shows file details like -rw-r--r-- 1 root root 140123456 Jul 28 dataset.zip\n",
        "# - If failed: \"No such file or directory\" error\n",
        "# - The file size should match what you saw in your Google Drive\n",
        "\n",
        "# Alternative: List all ZIP files in the workspace\n",
        "!ls -la /content/*.zip\n",
        "\n",
        "# üìù The asterisk (*) is a wildcard:\n",
        "# - *.zip means \"any filename ending with .zip\"\n",
        "# - Shows all ZIP files in the /content/ directory\n",
        "# - Useful if you have multiple ZIP files\n",
        "\n",
        "# =============================================================================\n",
        "# üì¶ STEP 5: EXTRACT (UNZIP) THE FILE\n",
        "# =============================================================================\n",
        "\n",
        "# Extract the contents of the ZIP file\n",
        "# unzip = command to extract ZIP archives\n",
        "# -o = overwrite existing files without asking for confirmation\n",
        "# /content/dataset.zip = the ZIP file to extract (source)\n",
        "# -d /content/ = destination directory where files will be extracted\n",
        "!unzip -o /content/dataset.zip -d /content/\n",
        "\n",
        "# üìù Breaking down the flags:\n",
        "# -o (overwrite): If files already exist, replace them without asking\n",
        "# -d (directory): Specifies where to extract the files\n",
        "# Without -d, files would extract to the current directory\n",
        "\n",
        "# üìù What happens during extraction:\n",
        "# - The ZIP file is opened and read\n",
        "# - All files/folders inside are created in /content/\n",
        "# - You'll see output like \"inflating: file1.txt\" for each extracted file\n",
        "# - The original ZIP file remains unchanged (it's not deleted)\n",
        "\n",
        "# Alternative extraction commands you might see:\n",
        "\n",
        "# Extract without overwriting (asks for confirmation if files exist)\n",
        "# !unzip /content/dataset.zip -d /content/\n",
        "\n",
        "# Just see what's inside the ZIP without extracting\n",
        "# !unzip -l /content/dataset.zip\n",
        "\n",
        "# Test the ZIP file integrity without extracting\n",
        "# !unzip -t /content/dataset.zip\n",
        "\n",
        "# =============================================================================\n",
        "# üìã STEP 6: CHECK WHAT WAS EXTRACTED\n",
        "# =============================================================================\n",
        "\n",
        "# List all contents of the workspace to see what was extracted\n",
        "!ls -la /content/\n",
        "\n",
        "# üìù What you'll see:\n",
        "# - All the files and folders that were inside your ZIP\n",
        "# - The original dataset.zip file (still there)\n",
        "# - sample_data/ folder (default Colab folder, ignore this)\n",
        "# - drive/ folder (your mounted Google Drive, ignore this)\n",
        "\n",
        "# More detailed view of extracted contents\n",
        "!ls -laR /content/\n",
        "\n",
        "# üìù The -R flag means \"recursive\":\n",
        "# - Shows contents of all subdirectories too\n",
        "# - Useful if your ZIP contained nested folders\n",
        "# - Can be overwhelming for large datasets\n",
        "\n",
        "# =============================================================================\n",
        "# üßπ STEP 7: CLEAN UP (OPTIONAL BUT RECOMMENDED)\n",
        "# =============================================================================\n",
        "\n",
        "# Remove the ZIP file to save space (since we've extracted everything)\n",
        "# rm = remove (delete) command\n",
        "!rm /content/dataset.zip\n",
        "\n",
        "# üìù Why delete the ZIP file:\n",
        "# - Saves disk space (ZIP file is now redundant)\n",
        "# - Colab has limited storage space\n",
        "# - We have all the contents extracted, so ZIP file is no longer needed\n",
        "# - You can always re-copy from Google Drive if needed\n",
        "\n",
        "# üìù BE CAREFUL: rm permanently deletes files!\n",
        "# - No \"recycle bin\" in Linux/Colab\n",
        "# - Make sure you've successfully extracted before deleting\n",
        "# - The original ZIP is still safe in your Google Drive\n",
        "\n",
        "# =============================================================================\n",
        "# üîç STEP 8: VERIFY EVERYTHING IS WORKING\n",
        "# =============================================================================\n",
        "\n",
        "# Final check - list everything to confirm setup is complete\n",
        "!ls -la /content/\n",
        "\n",
        "# üìù What you should see now:\n",
        "# - Your extracted files and folders\n",
        "# - NO dataset.zip (if you deleted it in step 7)\n",
        "# - drive/ folder (your Google Drive mount)\n",
        "# - sample_data/ folder (default Colab folder)\n",
        "\n",
        "# Check the total size of your extracted data\n",
        "!du -sh /content/\n",
        "\n",
        "# üìù du command explanation:\n",
        "# du = disk usage (shows how much space files use)\n",
        "# -s = summary (show total size, not individual files)\n",
        "# -h = human-readable (shows sizes like 1.2GB instead of 1200000000)"
      ],
      "metadata": {
        "id": "1OnO-cocdO6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning using trained model(Lane detection)"
      ],
      "metadata": {
        "id": "4_BQWaT_fEBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
        "!pip install roboflow ultralytics opencv-python\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from google.colab import files  # ÌååÏùº ÏóÖÎ°úÎìúÎ•º ÏúÑÌïú import Ï∂îÍ∞Ä\n",
        "# RoboflowÏóêÏÑú Ïù¥ÎØ∏ ÌõàÎ†®Îêú Î™®Îç∏ÏùÑ Î∂àÎü¨ÏôÄÏÑú ÏÇ¨Ïö©\n",
        "#ÏòÅÏÉÅÏóêÏÑú ÌîÑÎ†àÏûÑÏùÑ Ï∂îÏ∂úÌïòÏó¨ **Ï∂îÎ°†(inference)**Îßå ÏàòÌñâ\n",
        "import cv2\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "\n",
        "def load_lane_detection_model():\n",
        "    \"\"\"\n",
        "    Ï∞®ÏÑ† Í∞êÏßÄ Î™®Îç∏ Î°úÎìú\n",
        "    \"\"\"\n",
        "    rf = Roboflow(api_key=\"JwvZQEBhBR5uPrwepqQW\")\n",
        "    #project = rf.workspace().project(\"0722_labeling-usrpl/1\")\n",
        "    project = rf.workspace().project(\"0722_labeling-usrpl\")\n",
        "    model = project.version(1).model\n",
        "    print(\"üõ£Ô∏è Ï∞®ÏÑ† Í∞êÏßÄ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å!\")\n",
        "    return model\n",
        "\n",
        "def upload_video_files():\n",
        "    \"\"\"\n",
        "    PCÏóêÏÑú ÎπÑÎîîÏò§ ÌååÏùº ÏóÖÎ°úÎìú\n",
        "    \"\"\"\n",
        "    print(\"\\nüé¨ Please select your video files to upload:\")\n",
        "    uploaded = files.upload()  # Returns a dictionary: {filename: file_content}\n",
        "\n",
        "    # Check if any files were actually uploaded\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No files uploaded!\")\n",
        "        return None  # Return None if no files uploaded\n",
        "\n",
        "    print(f\"\\n‚úÖ Uploaded {len(uploaded)} file(s)\")\n",
        "\n",
        "    # ÏóÖÎ°úÎìúÎêú ÌååÏùº Ï§ë Ï≤´ Î≤àÏß∏ ÎπÑÎîîÏò§ ÌååÏùº Î∞òÌôò\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv')):\n",
        "            print(f\"üìÅ Using video file: {filename}\")\n",
        "            return filename\n",
        "\n",
        "    print(\"‚ùå No valid video files found in upload!\")\n",
        "    return None\n",
        "\n",
        "def detect_lanes_in_video(video_path, model, output_path=\"lane_detection_result.mp4\"):\n",
        "    \"\"\"\n",
        "    ÏòÅÏÉÅÏóêÏÑú Ï∞®ÏÑ† Í∞êÏßÄ\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # ÏòÅÏÉÅ Ï†ïÎ≥¥\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"üé¨ ÏòÅÏÉÅ Ï†ïÎ≥¥: {width}x{height}, {fps}fps, {total_frames}ÌîÑÎ†àÏûÑ\")\n",
        "\n",
        "    # Ï∂úÎ†• ÏÑ§Ï†ï\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    lane_detections = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Îß§ 3ÌîÑÎ†àÏûÑÎßàÎã§ Ï∞®ÏÑ† Í∞êÏßÄ (Îçî ÏûêÏ£º)\n",
        "        if frame_count % 2 == 0:\n",
        "            try:\n",
        "                temp_img_path = \"temp_frame.jpg\"\n",
        "                cv2.imwrite(temp_img_path, frame)\n",
        "\n",
        "                # Ï∞®ÏÑ† Í∞êÏßÄ (ÎÇÆÏùÄ Ïã†Î¢∞ÎèÑ)\n",
        "                prediction = model.predict(temp_img_path, confidence=30, overlap=50)\n",
        "                predictions = prediction.json()['predictions']\n",
        "\n",
        "                frame_lanes = len(predictions)\n",
        "                lane_detections += frame_lanes\n",
        "\n",
        "                if frame_lanes > 0:\n",
        "                    print(f\"üõ£Ô∏è ÌîÑÎ†àÏûÑ {frame_count}: {frame_lanes}Í∞ú Ï∞®ÏÑ† Í∞êÏßÄ\")\n",
        "\n",
        "                # Ï∞®ÏÑ† Í∑∏Î¶¨Í∏∞\n",
        "                for lane in predictions:\n",
        "                    x1 = int(lane['x'] - lane['width']/2)\n",
        "                    y1 = int(lane['y'] - lane['height']/2)\n",
        "                    x2 = int(lane['x'] + lane['width']/2)\n",
        "                    y2 = int(lane['y'] + lane['height']/2)\n",
        "\n",
        "                    # Ï∞®ÏÑ†ÏùÄ Î≥¥ÎùºÏÉâÏúºÎ°ú ÌëúÏãú\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
        "\n",
        "                    # ÎùºÎ≤®\n",
        "                    label = f\"Lane: {lane['confidence']:.2f}\"\n",
        "                    cv2.rectangle(frame, (x1, y1-30), (x1+150, y1), (255, 0, 255), -1)\n",
        "                    cv2.putText(frame, label, (x1+5, y1-8),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "                if os.path.exists(temp_img_path):\n",
        "                    os.remove(temp_img_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå ÌîÑÎ†àÏûÑ {frame_count} Ï≤òÎ¶¨ Ïò§Î•ò: {e}\")\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "        # ÏßÑÌñâÏÉÅÌô©\n",
        "        if frame_count % 150 == 0:\n",
        "            progress = (frame_count / total_frames) * 100\n",
        "            print(f\"üìä ÏßÑÌñâÎ•†: {progress:.1f}% (Ï¥ù Ï∞®ÏÑ† Í∞êÏßÄ: {lane_detections}Í∞ú)\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"‚úÖ ÏôÑÎ£å! Ï¥ù {lane_detections}Í∞ú Ï∞®ÏÑ† Í∞êÏßÄ\")\n",
        "    print(f\"üé• Í≤∞Í≥º: {output_path}\")\n",
        "\n",
        "def main_lane_detection():\n",
        "    \"\"\"\n",
        "    Ï∞®ÏÑ† Í∞êÏßÄ Î©îÏù∏ Ìï®Ïàò\n",
        "    \"\"\"\n",
        "    print(\"üöó ÎπÑÎîîÏò§ ÌååÏùº ÏóÖÎ°úÎìú ÏãúÏûë...\")\n",
        "    video_path = upload_video_files()\n",
        "\n",
        "    if video_path:\n",
        "        print(f\"üìÅ ÏòÅÏÉÅ ÌååÏùº: {video_path}\")\n",
        "\n",
        "        print(\"üõ£Ô∏è Ï∞®ÏÑ† Í∞êÏßÄ Î™®Îç∏ Î°úÎìú Ï§ë...\")\n",
        "        try:\n",
        "            model = load_lane_detection_model()\n",
        "\n",
        "            print(\"üîç Ï∞®ÏÑ† Í∞êÏßÄ ÏãúÏûë...\")\n",
        "            detect_lanes_in_video(video_path, model, \"lane_detection_result.mp4\")\n",
        "\n",
        "            print(\"üéâ Ï∞®ÏÑ† Í∞êÏßÄ ÏôÑÎ£å!\")\n",
        "            print(\"üì∫ Í≤∞Í≥º ÏòÅÏÉÅ: lane_detection_result.mp4\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}\")\n",
        "            print(\"üîë API ÌÇ§Î•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî!\")\n",
        "    else:\n",
        "        print(\"‚ùå ÏóÖÎ°úÎìúÎêú ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
        "\n",
        "def test_with_webcam_url():\n",
        "    \"\"\"\n",
        "    Roboflow Visualize ÌéòÏù¥ÏßÄÏùò 'Paste YouTube or Image URL' Í∏∞Îä• ÏÇ¨Ïö©\n",
        "    \"\"\"\n",
        "    print(\"üåê Ïõπ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏:\")\n",
        "    print(\"1. Roboflow Visualize ÌéòÏù¥ÏßÄÏóêÏÑú\")\n",
        "    print(\"2. 'Paste YouTube or Image URL' ÏûÖÎ†•Ï∞ΩÏóê\")\n",
        "    print(\"3. YouTube URL Î∂ôÏó¨ÎÑ£Í∏∞\")\n",
        "    print(\"4. Ï∞®ÏÑ†Ïù¥ Ïûò Í∞êÏßÄÎêòÎäîÏßÄ ÌôïÏù∏\")\n",
        "\n",
        "# Ïã§Ìñâ\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üõ£Ô∏è Ï∞®ÏÑ† Í∞êÏßÄ Î™®ÎìúÎ°ú Î≥ÄÍ≤Ω!\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"üí° Ïù¥ Î™®Îç∏ÏùÄ Ï∞®ÏÑ†(lane)ÏùÑ Í∞êÏßÄÌïòÎäî Î™®Îç∏ÏûÖÎãàÎã§.\")\n",
        "    print(\"üìπ ÎìúÎùºÏù¥Îπô ÏòÅÏÉÅÏù¥ÎÇò ÎèÑÎ°ú ÏòÅÏÉÅÏóêÏÑú Í∞ÄÏû• Ïûò ÏûëÎèôÌï©ÎãàÎã§.\")\n",
        "    print()\n",
        "    print(\"üöÄ Ïã§Ìñâ Î∞©Î≤ï:\")\n",
        "    print(\"1. API ÌÇ§ ÏûÖÎ†•\")\n",
        "    print(\"2. main_lane_detection() Ïã§Ìñâ\")\n",
        "    print(\"3. PCÏóêÏÑú ÎπÑÎîîÏò§ ÌååÏùº ÏÑ†ÌÉù\")\n",
        "    print(\"4. ÎòêÎäî Roboflow ÏõπÏóêÏÑú test_with_webcam_url() Î∞©Î≤ï ÏãúÎèÑ\")\n",
        "    print(\"=\" * 50)\n",
        "    main_lane_detection()"
      ],
      "metadata": {
        "id": "HnZ72k2bz3tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning using trained model(General object detection)"
      ],
      "metadata": {
        "id": "-NJs885dfRvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò (REVISED: Added YOLOv8 for comprehensive object detection)\n",
        "!pip install roboflow ultralytics opencv-python\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "import numpy as np\n",
        "from ultralytics import YOLO  # REVISED: Using YOLOv8 for multi-object detection\n",
        "from google.colab import files  # ÌååÏùº ÏóÖÎ°úÎìúÎ•º ÏúÑÌïú import Ï∂îÍ∞Ä\n",
        "# REVISED: Now supports detection of traffic signs, traffic lights, crosswalks, lanes, humans and vehicles\n",
        "# ÏòÅÏÉÅÏóêÏÑú ÌîÑÎ†àÏûÑÏùÑ Ï∂îÏ∂úÌïòÏó¨ **Îã§Ï§ë Í∞ùÏ≤¥ Ï∂îÎ°†(multi-object inference)** ÏàòÌñâ\n",
        "import cv2\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# REVISED: Added comprehensive object detection configuration\n",
        "DETECTION_CONFIG = {\n",
        "    # Object colors for visualization (BGR format)\n",
        "    'colors': {\n",
        "        'person': (0, 255, 0),          # Green for humans\n",
        "        'car': (255, 0, 0),             # Blue for cars\n",
        "        'truck': (255, 0, 150),         # Purple for trucks\n",
        "        'bus': (255, 100, 0),           # Orange for buses\n",
        "        'motorcycle': (0, 255, 255),    # Yellow for motorcycles\n",
        "        'bicycle': (255, 255, 0),       # Cyan for bicycles\n",
        "        'traffic light': (0, 0, 255),   # Red for traffic lights\n",
        "        'stop sign': (0, 150, 255),     # Orange-red for stop signs\n",
        "        'lane': (255, 0, 255),          # Magenta for lanes\n",
        "        'crosswalk': (255, 255, 255),   # White for crosswalks\n",
        "        'default': (128, 128, 128)      # Gray for other objects\n",
        "    },\n",
        "\n",
        "    # Traffic-related object classes from YOLO\n",
        "    'traffic_classes': [\n",
        "        'person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck',\n",
        "        'traffic light', 'stop sign'\n",
        "    ],\n",
        "\n",
        "    # Confidence thresholds for different object types\n",
        "    'confidence_thresholds': {\n",
        "        'yolo': 0.5,      # 50% confidence for YOLO detections\n",
        "        'lane': 0.3       # 30% confidence for lane detections\n",
        "    }\n",
        "}\n",
        "\n",
        "def load_detection_models():\n",
        "    \"\"\"\n",
        "    REVISED: Load both YOLOv8 and lane detection models\n",
        "    Îã§Ï§ë Î™®Îç∏ Î°úÎìú - YOLO(ÏùºÎ∞ò Í∞ùÏ≤¥) + Roboflow(Ï∞®ÏÑ†)\n",
        "    \"\"\"\n",
        "    print(\"ü§ñ Loading comprehensive detection models...\")\n",
        "\n",
        "    # 1. Load YOLOv8 model for general traffic objects\n",
        "    print(\"  üì¶ Loading YOLOv8 model for traffic objects...\")\n",
        "    yolo_model = YOLO('yolov8n.pt')  # Nano version for speed, use 'yolov8s.pt' for better accuracy\n",
        "    print(\"  ‚úÖ YOLOv8 model loaded successfully!\")\n",
        "\n",
        "    # 2. Load Roboflow model for lane detection\n",
        "    print(\"  üõ£Ô∏è  Loading specialized lane detection model...\")\n",
        "    try:\n",
        "        rf = Roboflow(api_key=\"JwvZQEBhBR5uPrwepqQW\")\n",
        "        project = rf.workspace().project(\"0722_labeling-usrpl\")\n",
        "        lane_model = project.version(1).model\n",
        "        print(\"  ‚úÖ Lane detection model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è  Lane model loading failed: {e}\")\n",
        "        print(\"  üìù Continuing with YOLOv8 only...\")\n",
        "        lane_model = None\n",
        "\n",
        "    print(\"üéØ All available models loaded!\")\n",
        "    return yolo_model, lane_model\n",
        "\n",
        "def upload_video_files():\n",
        "    \"\"\"\n",
        "    PCÏóêÏÑú ÎπÑÎîîÏò§ ÌååÏùº ÏóÖÎ°úÎìú (Unchanged)\n",
        "    \"\"\"\n",
        "    print(\"\\nüé¨ Please select your video files to upload:\")\n",
        "    uploaded = files.upload()  # Returns a dictionary: {filename: file_content}\n",
        "\n",
        "    # Check if any files were actually uploaded\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No files uploaded!\")\n",
        "        return None  # Return None if no files uploaded\n",
        "\n",
        "    print(f\"\\n‚úÖ Uploaded {len(uploaded)} file(s)\")\n",
        "\n",
        "    # ÏóÖÎ°úÎìúÎêú ÌååÏùº Ï§ë Ï≤´ Î≤àÏß∏ ÎπÑÎîîÏò§ ÌååÏùº Î∞òÌôò\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv')):\n",
        "            print(f\"üìÅ Using video file: {filename}\")\n",
        "            return filename\n",
        "\n",
        "    print(\"‚ùå No valid video files found in upload!\")\n",
        "    return None\n",
        "\n",
        "def detect_yolo_objects(frame, yolo_model):\n",
        "    \"\"\"\n",
        "    REVISED: YOLOv8ÏùÑ ÏÇ¨Ïö©Ìïú ÍµêÌÜµ Í∞ùÏ≤¥ Í∞êÏßÄ\n",
        "    Detect traffic-related objects using YOLOv8\n",
        "    \"\"\"\n",
        "    detections = []\n",
        "\n",
        "    try:\n",
        "        # Run YOLOv8 inference\n",
        "        results = yolo_model(frame, conf=DETECTION_CONFIG['confidence_thresholds']['yolo'])\n",
        "\n",
        "        # Parse results\n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            if boxes is not None:\n",
        "                for box in boxes:\n",
        "                    # Get class name\n",
        "                    class_id = int(box.cls[0])\n",
        "                    class_name = yolo_model.names[class_id]\n",
        "\n",
        "                    # Only keep traffic-related objects\n",
        "                    if class_name in DETECTION_CONFIG['traffic_classes']:\n",
        "                        confidence = float(box.conf[0])\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "\n",
        "                        detections.append({\n",
        "                            'class': class_name,\n",
        "                            'confidence': confidence,\n",
        "                            'bbox': (int(x1), int(y1), int(x2), int(y2)),\n",
        "                            'type': 'yolo'\n",
        "                        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è YOLOv8 detection error: {e}\")\n",
        "\n",
        "    return detections\n",
        "\n",
        "def detect_lane_objects(frame, lane_model):\n",
        "    \"\"\"\n",
        "    REVISED: RoboflowÎ•º ÏÇ¨Ïö©Ìïú Ï∞®ÏÑ† Í∞êÏßÄ\n",
        "    Detect lanes using specialized Roboflow model\n",
        "    \"\"\"\n",
        "    detections = []\n",
        "\n",
        "    if lane_model is None:\n",
        "        return detections\n",
        "\n",
        "    try:\n",
        "        # Save frame temporarily for Roboflow processing\n",
        "        temp_img_path = \"temp_frame_lane.jpg\"\n",
        "        cv2.imwrite(temp_img_path, frame)\n",
        "\n",
        "        # Run lane detection\n",
        "        prediction = lane_model.predict(\n",
        "            temp_img_path,\n",
        "            confidence=int(DETECTION_CONFIG['confidence_thresholds']['lane'] * 100),\n",
        "            overlap=50\n",
        "        )\n",
        "        predictions = prediction.json()['predictions']\n",
        "\n",
        "        # Parse lane detections\n",
        "        for lane in predictions:\n",
        "            x1 = int(lane['x'] - lane['width']/2)\n",
        "            y1 = int(lane['y'] - lane['height']/2)\n",
        "            x2 = int(lane['x'] + lane['width']/2)\n",
        "            y2 = int(lane['y'] + lane['height']/2)\n",
        "\n",
        "            detections.append({\n",
        "                'class': 'lane',\n",
        "                'confidence': lane['confidence'] / 100.0,  # Convert to 0-1 scale\n",
        "                'bbox': (x1, y1, x2, y2),\n",
        "                'type': 'lane'\n",
        "            })\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_img_path):\n",
        "            os.remove(temp_img_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Lane detection error: {e}\")\n",
        "\n",
        "    return detections\n",
        "\n",
        "def draw_detections(frame, detections):\n",
        "    \"\"\"\n",
        "    REVISED: Î™®Îì† Í∞êÏßÄÎêú Í∞ùÏ≤¥Î•º ÌîÑÎ†àÏûÑÏóê Í∑∏Î¶¨Í∏∞\n",
        "    Draw all detected objects on frame with different colors\n",
        "    \"\"\"\n",
        "    detection_counts = {}\n",
        "\n",
        "    for detection in detections:\n",
        "        class_name = detection['class']\n",
        "        confidence = detection['confidence']\n",
        "        x1, y1, x2, y2 = detection['bbox']\n",
        "\n",
        "        # Count detections by class\n",
        "        detection_counts[class_name] = detection_counts.get(class_name, 0) + 1\n",
        "\n",
        "        # Get color for this object class\n",
        "        color = DETECTION_CONFIG['colors'].get(class_name, DETECTION_CONFIG['colors']['default'])\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "        # Prepare label\n",
        "        label = f\"{class_name}: {confidence:.2f}\"\n",
        "\n",
        "        # Calculate label background size\n",
        "        (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "\n",
        "        # Draw label background\n",
        "        cv2.rectangle(frame, (x1, y1-25), (x1 + label_width + 10, y1), color, -1)\n",
        "\n",
        "        # Draw label text\n",
        "        cv2.putText(frame, label, (x1 + 5, y1 - 5),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "    return detection_counts\n",
        "\n",
        "def detect_comprehensive_objects_in_video(video_path, yolo_model, lane_model, output_path=\"comprehensive_detection_result.mp4\"):\n",
        "    \"\"\"\n",
        "    REVISED: ÏòÅÏÉÅÏóêÏÑú Ï¢ÖÌï©Ï†ÅÏù∏ ÎèÑÎ°ú Í∞ùÏ≤¥ Í∞êÏßÄ\n",
        "    Comprehensive road object detection in video\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # ÏòÅÏÉÅ Ï†ïÎ≥¥ ÌôïÏù∏\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"üé¨ Video info: {width}x{height}, {fps}fps, {total_frames} frames\")\n",
        "\n",
        "    # Ï∂úÎ†• ÏÑ§Ï†ï\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # REVISED: Tracking variables for comprehensive detection\n",
        "    frame_count = 0\n",
        "    total_detections = {\n",
        "        'person': 0, 'car': 0, 'truck': 0, 'bus': 0, 'motorcycle': 0,\n",
        "        'bicycle': 0, 'traffic light': 0, 'stop sign': 0, 'lane': 0\n",
        "    }\n",
        "\n",
        "    print(\"üîç Starting comprehensive object detection...\")\n",
        "    print(\"üìä Detection progress:\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        frame_detections = []\n",
        "\n",
        "        # REVISED: Process every 3rd frame for efficiency (can be adjusted)\n",
        "        if frame_count % 3 == 0:\n",
        "            print(f\"    üîé Processing frame {frame_count}/{total_frames}...\")\n",
        "\n",
        "            # 1. YOLO Object Detection\n",
        "            print(\"      üöó Running traffic object detection...\")\n",
        "            yolo_detections = detect_yolo_objects(frame, yolo_model)\n",
        "            frame_detections.extend(yolo_detections)\n",
        "\n",
        "            # 2. Lane Detection\n",
        "            print(\"      üõ£Ô∏è  Running lane detection...\")\n",
        "            lane_detections = detect_lane_objects(frame, lane_model)\n",
        "            frame_detections.extend(lane_detections)\n",
        "\n",
        "            # 3. Draw all detections\n",
        "            frame_counts = draw_detections(frame, frame_detections)\n",
        "\n",
        "            # 4. Update total counts\n",
        "            for obj_class, count in frame_counts.items():\n",
        "                if obj_class in total_detections:\n",
        "                    total_detections[obj_class] += count\n",
        "\n",
        "            # 5. Print frame summary\n",
        "            if frame_detections:\n",
        "                detected_objects = list(frame_counts.keys())\n",
        "                print(f\"      ‚úÖ Frame {frame_count}: Found {len(frame_detections)} objects - {', '.join(detected_objects)}\")\n",
        "\n",
        "        # Write frame to output video\n",
        "        out.write(frame)\n",
        "\n",
        "        # REVISED: Progress reporting every 5%\n",
        "        if frame_count % max(1, total_frames // 20) == 0:\n",
        "            progress = (frame_count / total_frames) * 100\n",
        "            print(f\"\\nüìà Progress: {progress:.1f}% complete\")\n",
        "            print(\"üéØ Detection summary so far:\")\n",
        "            for obj_type, count in total_detections.items():\n",
        "                if count > 0:\n",
        "                    print(f\"    {obj_type}: {count}\")\n",
        "            print()\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # REVISED: Final comprehensive summary\n",
        "    print(\"üéâ Comprehensive detection completed!\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"üìä FINAL DETECTION SUMMARY:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for obj_type, count in total_detections.items():\n",
        "        if count > 0:\n",
        "            print(f\"üî∏ {obj_type.title()}: {count} detections\")\n",
        "\n",
        "    total_objects = sum(total_detections.values())\n",
        "    print(f\"\\nüéØ Total objects detected: {total_objects}\")\n",
        "    print(f\"üé• Output video: {output_path}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "def main_comprehensive_detection():\n",
        "    \"\"\"\n",
        "    REVISED: Ï¢ÖÌï©Ï†ÅÏù∏ ÎèÑÎ°ú Í∞ùÏ≤¥ Í∞êÏßÄ Î©îÏù∏ Ìï®Ïàò\n",
        "    Main function for comprehensive road object detection\n",
        "    \"\"\"\n",
        "    print(\"üöó Starting comprehensive road object detection...\")\n",
        "    print(\"üéØ Will detect: Traffic signs, lights, crosswalks, lanes, humans, and vehicles\")\n",
        "\n",
        "    # Step 1: Upload video file\n",
        "    video_path = upload_video_files()\n",
        "\n",
        "    if video_path:\n",
        "        print(f\"üìÅ Video file: {video_path}\")\n",
        "\n",
        "        # Step 2: Load detection models\n",
        "        print(\"\\nü§ñ Loading detection models...\")\n",
        "        try:\n",
        "            yolo_model, lane_model = load_detection_models()\n",
        "\n",
        "            # Step 3: Run comprehensive detection\n",
        "            print(\"\\nüîç Starting comprehensive object detection...\")\n",
        "            detect_comprehensive_objects_in_video(\n",
        "                video_path, yolo_model, lane_model,\n",
        "                \"comprehensive_detection_result.mp4\"\n",
        "            )\n",
        "\n",
        "            print(\"\\nüéâ Comprehensive detection completed!\")\n",
        "            print(\"üì∫ Result video: comprehensive_detection_result.mp4\")\n",
        "            print(\"\\nüé® Color coding:\")\n",
        "            for obj_type, color in DETECTION_CONFIG['colors'].items():\n",
        "                if obj_type != 'default':\n",
        "                    print(f\"  {obj_type}: {color}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Model loading failed: {e}\")\n",
        "            print(\"üîë Please check your API key and internet connection!\")\n",
        "    else:\n",
        "        print(\"‚ùå No video file uploaded.\")\n",
        "\n",
        "def test_with_webcam_url():\n",
        "    \"\"\"\n",
        "    Roboflow Visualize ÌéòÏù¥ÏßÄÏùò 'Paste YouTube or Image URL' Í∏∞Îä• ÏÇ¨Ïö© (Unchanged)\n",
        "    \"\"\"\n",
        "    print(\"üåê Ïõπ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏:\")\n",
        "    print(\"1. Roboflow Visualize ÌéòÏù¥ÏßÄÏóêÏÑú\")\n",
        "    print(\"2. 'Paste YouTube or Image URL' ÏûÖÎ†•Ï∞ΩÏóê\")\n",
        "    print(\"3. YouTube URL Î∂ôÏó¨ÎÑ£Í∏∞\")\n",
        "    print(\"4. Ï∞®ÏÑ†Ïù¥ Ïûò Í∞êÏßÄÎêòÎäîÏßÄ ÌôïÏù∏\")\n",
        "\n",
        "# Ïã§Ìñâ\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üö¶ COMPREHENSIVE ROAD OBJECT DETECTION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üéØ DETECTABLE OBJECTS:\")\n",
        "    print(\"  üë§ Humans (persons)\")\n",
        "    print(\"  üöó Vehicles (cars, trucks, buses, motorcycles, bicycles)\")\n",
        "    print(\"  üö¶ Traffic lights\")\n",
        "    print(\"  üõë Traffic signs (stop signs)\")\n",
        "    print(\"  üõ£Ô∏è  Road lanes\")\n",
        "    print(\"  üö∂ Crosswalks (when visible)\")\n",
        "    print()\n",
        "    print(\"üé® VISUAL CODING:\")\n",
        "    print(\"  Different colors for different object types\")\n",
        "    print(\"  Confidence scores displayed for each detection\")\n",
        "    print()\n",
        "    print(\"üöÄ EXECUTION STEPS:\")\n",
        "    print(\"1. Upload your driving video file\")\n",
        "    print(\"2. Models will load automatically\")\n",
        "    print(\"3. Comprehensive detection will run\")\n",
        "    print(\"4. Results saved as comprehensive_detection_result.mp4\")\n",
        "    print(\"=\" * 60)\n",
        "    main_comprehensive_detection()"
      ],
      "metadata": {
        "id": "MJg4OpJVfITL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the contents of dataset.yaml"
      ],
      "metadata": {
        "id": "_NeRfugzfonO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. dataset.yaml ÌååÏùº ÎÇ¥Ïö© ÌôïÏù∏\n",
        "print(\"üìã dataset.yaml ÌååÏùº ÎÇ¥Ïö©:\")\n",
        "with open('/content/dataset/dataset.yaml', 'r') as f:\n",
        "    yaml_content = f.read()\n",
        "    print(yaml_content)"
      ],
      "metadata": {
        "id": "pqn9bfQjfm06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trasfer Learning(perception) using YOLO11 and pre-traind YOLO"
      ],
      "metadata": {
        "id": "NgsEQKJGfoLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for YOLO and video processing\n",
        "!pip install ultralytics yt-dlp\n",
        "\n",
        "# Import all necessary libraries\n",
        "from ultralytics import YOLO           # YOLO model for object detection\n",
        "import glob                            # File path pattern matching\n",
        "import cv2                             # OpenCV for video processing\n",
        "import numpy as np                     # Numerical operations\n",
        "from IPython.display import Video      # Display videos in Jupyter notebook\n",
        "import shutil                          # File operations (copy, move files)\n",
        "from google.colab import files         # File upload functionality for Google Colab\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: CREATE DATASET CONFIGURATION FILE\n",
        "# ============================================================================\n",
        "# This YAML configuration file defines the dataset structure for the custom model\n",
        "# It specifies where training/validation images are located and what classes exist\n",
        "yaml_fix = '''path: /content/dataset\n",
        "train: train/images\n",
        "val: valid/images\n",
        "names:\n",
        "  0: lane                # Class 0: Lane detection\n",
        "  1: traffic_sign        # Class 1: Traffic sign detection\n",
        "nc: 2'''                 # Number of classes (nc = number of classes)\n",
        "\n",
        "# Write the configuration to a file that YOLO can read\n",
        "with open('/content/dataset/dataset_fixed.yaml', 'w') as f:\n",
        "    f.write(yaml_fix)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: LOAD YOLO MODELS\n",
        "# ============================================================================\n",
        "print(\"ü§ñ Î™®Îç∏ Î°úÎìú Ï§ë...\")\n",
        "print(\"   - Í∏∞Î≥∏ YOLO Î™®Îç∏: ÏùºÎ∞òÏ†ÅÏù∏ 80Í∞ú ÌÅ¥ÎûòÏä§ Í∞ùÏ≤¥ ÌÉêÏßÄ (ÏÇ¨Îûå, Ï∞®Îüâ, Ïã†Ìò∏Îì± Îì±)\")\n",
        "print(\"   - Ïª§Ïä§ÌÖÄ YOLO Î™®Îç∏: ÌäπÎ≥ÑÌûà ÌïôÏäµÎêú 2Í∞ú ÌÅ¥ÎûòÏä§ ÌÉêÏßÄ (Ï∞®ÏÑ†, ÍµêÌÜµÌëúÏßÄÌåê)\")\n",
        "\n",
        "# Load the base YOLO11n model (pre-trained on COCO dataset with 80 classes)\n",
        "base_model = YOLO('yolo11n.pt')        # Detects: person, car, truck, traffic light, etc.\n",
        "\n",
        "# Load your custom trained model (specialized for 2 classes)\n",
        "custom_model = YOLO('/content/dataset/best.pt')  # Detects: lane, traffic_sign\n",
        "\n",
        "print(f\"üìã Í∏∞Î≥∏ Î™®Îç∏ ÌÅ¥ÎûòÏä§ Ïàò: {len(base_model.names)} (COCO Îç∞Ïù¥ÌÑ∞ÏÖã Í∏∞Î∞ò)\")\n",
        "print(f\"üìã Ïª§Ïä§ÌÖÄ Î™®Îç∏ ÌÅ¥ÎûòÏä§ Ïàò: {len(custom_model.names)} (Ï∞®ÏÑ†, ÍµêÌÜµÌëúÏßÄÌåê Ï†ÑÏö©)\")\n",
        "\n",
        "# Display what classes each model can detect\n",
        "print(f\"   üîç Í∏∞Î≥∏ Î™®Îç∏ ÌÅ¥ÎûòÏä§ ÏòàÏãú: {list(base_model.names.values())[:10]}...\")  # Show first 10 classes\n",
        "print(f\"   üîç Ïª§Ïä§ÌÖÄ Î™®Îç∏ ÌÅ¥ÎûòÏä§: {list(custom_model.names.values())}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: UPLOAD VIDEO FILES FROM PC\n",
        "# ============================================================================\n",
        "print(\"\\nüì• ÎπÑÎîîÏò§ ÌååÏùº ÏóÖÎ°úÎìú\")\n",
        "print(\"   PCÏóêÏÑú ÎπÑÎîîÏò§ ÌååÏùºÏùÑ ÏÑ†ÌÉùÌïòÏó¨ ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî\")\n",
        "print(\"   ÏßÄÏõê ÌòïÏãù: MP4, AVI, MOV, MKV Îì± ÎåÄÎ∂ÄÎ∂ÑÏùò ÎπÑÎîîÏò§ ÌòïÏãù\")\n",
        "\n",
        "# Open file upload dialog for users to select video files from their PC\n",
        "print(\"\\nüé¨ ÏóÖÎ°úÎìúÌï† ÎπÑÎîîÏò§ ÌååÏùºÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:\")\n",
        "uploaded = files.upload()  # Returns a dictionary: {filename: file_content}\n",
        "\n",
        "# Check if any files were actually uploaded\n",
        "if not uploaded:\n",
        "    print(\"‚ùå ÏóÖÎ°úÎìúÎêú ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§!\")\n",
        "    raise Exception(\"No files uploaded - ÏóÖÎ°úÎìúÎ•º Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî\")\n",
        "\n",
        "print(f\"\\n‚úÖ {len(uploaded)} Í∞úÏùò ÌååÏùºÏù¥ ÏóÖÎ°úÎìúÎêòÏóàÏäµÎãàÎã§\")\n",
        "\n",
        "# Get the first uploaded video file path\n",
        "# The uploaded files are automatically saved to the current directory\n",
        "uploaded_filenames = list(uploaded.keys())\n",
        "video_path = uploaded_filenames[0]  # Use the first uploaded file\n",
        "print(f\"üìπ Ï≤òÎ¶¨Ìï† ÎπÑÎîîÏò§: {video_path}\")\n",
        "print(f\"üìÅ ÌååÏùº ÌÅ¨Í∏∞: {len(uploaded[video_path]) / (1024*1024):.1f} MB\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: COMBINED INFERENCE FUNCTION\n",
        "# ============================================================================\n",
        "def combined_inference(video_path, output_path='/content/combined_result.mp4'):\n",
        "    \"\"\"\n",
        "    Combine results from both base YOLO and custom YOLO models\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to input video file\n",
        "        output_path: Path where the result video will be saved\n",
        "\n",
        "    Process:\n",
        "        1. Read video frame by frame\n",
        "        2. Run both models on each frame\n",
        "        3. Draw detection boxes with different colors\n",
        "        4. Save processed video\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nüé¨ Í≤∞Ìï©Îêú Ï∂îÎ°† ÏãúÏûë: {video_path}\")\n",
        "    print(\"   Îëê Í∞úÏùò YOLO Î™®Îç∏ÏùÑ ÎèôÏãúÏóê Ïã§ÌñâÌïòÏó¨ Î™®Îì† Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌï©ÎãàÎã§\")\n",
        "\n",
        "    # ========== Video Properties Setup ==========\n",
        "    cap = cv2.VideoCapture(video_path)           # Open video file for reading\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))         # Get original video frame rate\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   # Get video width in pixels\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Get video height in pixels\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Total number of frames\n",
        "\n",
        "    print(f\"   üìπ ÎπÑÎîîÏò§ Ï†ïÎ≥¥: {width}x{height} Ìï¥ÏÉÅÎèÑ, {fps} FPS, {total_frames} Ï¥ù ÌîÑÎ†àÏûÑ\")\n",
        "\n",
        "    # Setup output video writer with same properties as input video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')     # Video codec for MP4 format\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # ========== Frame Processing Loop ==========\n",
        "    frame_count = 0\n",
        "    print(f\"üîÑ ÏòÅÏÉÅ Ï≤òÎ¶¨ Ï§ë... (Ï¥ù {total_frames} ÌîÑÎ†àÏûÑ)\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()  # Read next frame from video\n",
        "        if not ret:              # If no more frames, break the loop\n",
        "            break\n",
        "\n",
        "        # ========== Run Both YOLO Models ==========\n",
        "        # Base YOLO inference: detects general objects (cars, people, traffic lights, etc.)\n",
        "        base_results = base_model(frame, verbose=False)    # verbose=False: suppress output messages\n",
        "\n",
        "        # Custom YOLO inference: detects specialized objects (lanes, traffic signs)\n",
        "        custom_results = custom_model(frame, verbose=False)\n",
        "\n",
        "        # ========== Prepare Frame for Annotation ==========\n",
        "        annotated_frame = frame.copy()  # Create a copy to draw on (preserve original)\n",
        "\n",
        "        # ========== Draw Base YOLO Results (BLUE boxes) ==========\n",
        "        if base_results[0].boxes is not None:  # Check if any objects were detected\n",
        "            for box in base_results[0].boxes:\n",
        "                # Extract bounding box coordinates (x1, y1) = top-left, (x2, y2) = bottom-right\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])        # Confidence score (0.0 to 1.0)\n",
        "                cls = int(box.cls[0])            # Class index number\n",
        "\n",
        "                # Only draw boxes with confidence > 30% to reduce false positives\n",
        "                if conf > 0.3:\n",
        "                    # Create label text with class name and confidence percentage\n",
        "                    label = f\"{base_model.names[cls]} {conf:.2f}\"\n",
        "\n",
        "                    # Draw blue rectangle around detected object\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Blue color (B, G, R)\n",
        "\n",
        "                    # Draw label text above the rectangle\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # ========== Draw Custom YOLO Results (RED boxes) ==========\n",
        "        if custom_results[0].boxes is not None:  # Check if any objects were detected\n",
        "            for box in custom_results[0].boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                # Only draw boxes with confidence > 30%\n",
        "                if conf > 0.3:\n",
        "                    # Create label text with class name and confidence\n",
        "                    label = f\"{custom_model.names[cls]} {conf:.2f}\"\n",
        "\n",
        "                    # Draw red rectangle around detected object\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Red color (B, G, R)\n",
        "\n",
        "                    # Draw label text above the rectangle\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # ========== Save Processed Frame ==========\n",
        "        out.write(annotated_frame)  # Write the annotated frame to output video\n",
        "        frame_count += 1\n",
        "\n",
        "        # Progress reporting every 30 frames to avoid overwhelming the console\n",
        "        if frame_count % 30 == 0:\n",
        "            progress_percent = (frame_count / total_frames) * 100\n",
        "            print(f\"   üîÑ ÏßÑÌñâÏÉÅÌô©: {frame_count}/{total_frames} ({progress_percent:.1f}%)\")\n",
        "\n",
        "    # ========== Cleanup and Finalize ==========\n",
        "    cap.release()   # Close input video file\n",
        "    out.release()   # Close output video file and finalize writing\n",
        "\n",
        "    print(f\"‚úÖ Í≤∞Ìï© Í≤∞Í≥º ÏòÅÏÉÅ Ï†ÄÏû• ÏôÑÎ£å: {output_path}\")\n",
        "    print(f\"üìä Ï≤òÎ¶¨Îêú Ï¥ù ÌîÑÎ†àÏûÑ: {frame_count}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: INDIVIDUAL MODEL INFERENCE RESULTS\n",
        "# ============================================================================\n",
        "print(\"\\nüîç Í∞úÎ≥Ñ Î™®Îç∏ Ï∂îÎ°† Í≤∞Í≥º ÏÉùÏÑ±:\")\n",
        "print(\"   Í∞Å Î™®Îç∏Ïùò Í∞úÎ≥Ñ ÏÑ±Îä•ÏùÑ ÌôïÏù∏ÌïòÍ∏∞ ÏúÑÌï¥ Îî∞Î°úÎî∞Î°ú Ïã§ÌñâÌï©ÎãàÎã§\")\n",
        "\n",
        "# ========== Custom Model Only Inference ==========\n",
        "print(\"\\n1Ô∏è‚É£ Ïª§Ïä§ÌÖÄ Î™®Îç∏ Îã®ÎèÖ Ïã§Ìñâ (Ï∞®ÏÑ†, ÍµêÌÜµÌëúÏßÄÌåêÎßå ÌÉêÏßÄ):\")\n",
        "print(\"   Ïù¥ Î™®Îç∏ÏùÄ ÌäπÎ≥ÑÌûà Ï∞®ÏÑ†Í≥º ÍµêÌÜµÌëúÏßÄÌåê ÌÉêÏßÄÎ•º ÏúÑÌï¥ ÌïôÏäµÎêòÏóàÏäµÎãàÎã§\")\n",
        "custom_results = custom_model(\n",
        "    video_path,                    # Input video path\n",
        "    save=True,                     # Save results automatically\n",
        "    project='/content',            # Project directory for saving\n",
        "    name='custom_only',            # Folder name for this run\n",
        "    conf=0.3                       # Minimum confidence threshold\n",
        ")\n",
        "\n",
        "# ========== Base Model Only Inference ==========\n",
        "print(\"\\n2Ô∏è‚É£ Í∏∞Î≥∏ Î™®Îç∏ Îã®ÎèÖ Ïã§Ìñâ (ÏùºÎ∞ò Í∞ùÏ≤¥Îì§ ÌÉêÏßÄ):\")\n",
        "print(\"   COCO Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú ÌïôÏäµÎêú Î™®Îç∏Î°ú 80Í∞ú ÌÅ¥ÎûòÏä§Ïùò ÏùºÎ∞òÏ†ÅÏù∏ Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌï©ÎãàÎã§\")\n",
        "base_results = base_model(\n",
        "    video_path,                    # Input video path\n",
        "    save=True,                     # Save results automatically\n",
        "    project='/content',            # Project directory for saving\n",
        "    name='base_only',              # Folder name for this run\n",
        "    conf=0.3                       # Minimum confidence threshold\n",
        ")\n",
        "\n",
        "# ========== Combined Model Inference ==========\n",
        "print(\"\\n3Ô∏è‚É£ Í≤∞Ìï© Î™®Îç∏ Ïã§Ìñâ (Î™®Îì† Í∞ùÏ≤¥ ÎèôÏãú ÌÉêÏßÄ):\")\n",
        "print(\"   Îëê Î™®Îç∏ÏùÑ ÎèôÏãúÏóê Ïã§ÌñâÌïòÏó¨ Î™®Îì† Ï¢ÖÎ•òÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌï©ÎãàÎã§\")\n",
        "print(\"   ÌååÎûÄÏÉâ Î∞ïÏä§: Í∏∞Î≥∏ Î™®Îç∏ Í≤∞Í≥º, Îπ®Í∞ÑÏÉâ Î∞ïÏä§: Ïª§Ïä§ÌÖÄ Î™®Îç∏ Í≤∞Í≥º\")\n",
        "combined_inference(video_path, '/content/combined_result.mp4')\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: ORGANIZE RESULT FILES\n",
        "# ============================================================================\n",
        "print(\"\\nüìÅ Í≤∞Í≥º ÌååÏùº Ï†ïÎ¶¨ Ï§ë...\")\n",
        "print(\"   ÏÉùÏÑ±Îêú Í≤∞Í≥º ÌååÏùºÎì§ÏùÑ ÌëúÏ§Ä Ïù¥Î¶ÑÏúºÎ°ú Î≥µÏÇ¨ÌïòÏó¨ ÏâΩÍ≤å Ï†ëÍ∑ºÌï† Ïàò ÏûàÎèÑÎ°ù Ìï©ÎãàÎã§\")\n",
        "\n",
        "# Define mapping of standard names to actual generated file paths\n",
        "result_files = {\n",
        "    'custom_result.mp4': glob.glob('/content/custom_only/*.avi') + glob.glob('/content/custom_only/*.mp4'),\n",
        "    'base_result.mp4': glob.glob('/content/base_only/*.avi') + glob.glob('/content/base_only/*.mp4'),\n",
        "    'final_combined_result.mp4': ['/content/combined_result.mp4']\n",
        "}\n",
        "\n",
        "print(\"\\nüìã Í≤∞Í≥º ÌååÏùºÎì§:\")\n",
        "for standard_name, file_list in result_files.items():\n",
        "    if file_list and file_list[0]:  # Check if files exist\n",
        "        try:\n",
        "            # Copy the first found file to a standard name\n",
        "            shutil.copy(file_list[0], f'/content/{standard_name}')\n",
        "            print(f\"‚úÖ {standard_name} ÏÉùÏÑ± ÏôÑÎ£å (ÏõêÎ≥∏: {file_list[0]})\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {standard_name} ÏÉùÏÑ± Ïã§Ìå®: {e}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è {standard_name} Ìï¥Îãπ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: MODEL PERFORMANCE EVALUATION\n",
        "# ============================================================================\n",
        "print(\"\\nüìä Ïª§Ïä§ÌÖÄ Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä:\")\n",
        "print(\"   Validation Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏Ïùò Ï†ïÌôïÎèÑÎ•º Ï∏°Ï†ïÌï©ÎãàÎã§\")\n",
        "print(\"   mAP50ÏùÄ IoU 0.5ÏóêÏÑúÏùò ÌèâÍ∑† Ï†ïÎ∞ÄÎèÑÎ°ú, ÎÜíÏùÑÏàòÎ°ù Ï¢ãÏùÄ ÏÑ±Îä•ÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§\")\n",
        "\n",
        "try:\n",
        "    # Evaluate custom model performance on validation dataset\n",
        "    metrics = custom_model.val(data='/content/dataset/dataset_fixed.yaml')\n",
        "    print(f\"üéØ mAP50 (Mean Average Precision): {metrics.box.map50:.4f}\")\n",
        "    print(f\"   Ìï¥ÏÑù: {metrics.box.map50:.1%} Ï†ïÌôïÎèÑÎ°ú Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌï©ÎãàÎã§\")\n",
        "\n",
        "    # Additional metrics if available\n",
        "    if hasattr(metrics.box, 'map'):\n",
        "        print(f\"üéØ mAP50-95 (Ï†ÑÏ≤¥ IoU Î≤îÏúÑ): {metrics.box.map:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Î™®Îç∏ ÌèâÍ∞Ä Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}\")\n",
        "    print(\"   Îç∞Ïù¥ÌÑ∞ÏÖã ÌååÏùºÏù¥ ÏóÜÍ±∞ÎÇò Í≤ΩÎ°úÍ∞Ä ÏûòÎ™ªÎêòÏóàÏùÑ Ïàò ÏûàÏäµÎãàÎã§\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: DISPLAY FINAL RESULTS\n",
        "# ============================================================================\n",
        "print(\"\\nüé¨ ÏµúÏ¢Ö Í≤∞Ìï© Í≤∞Í≥º ÏòÅÏÉÅ:\")\n",
        "print(\"   Î™®Îì† Í∞ùÏ≤¥Í∞Ä ÌÉêÏßÄÎêú ÏµúÏ¢Ö Í≤∞Í≥ºÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî\")\n",
        "\n",
        "try:\n",
        "    # Display the final combined result video\n",
        "    Video('/content/final_combined_result.mp4', width=800)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ÎπÑÎîîÏò§ ÌëúÏãú Ïò§Î•ò: {e}\")\n",
        "    print(\"   ÌååÏùºÏù¥ ÏÉùÏÑ±ÎêòÏóàÏßÄÎßå ÌëúÏãúÌï† Ïàò ÏóÜÏäµÎãàÎã§. ÌååÏùºÏùÑ ÏßÅÏ†ë Îã§Ïö¥Î°úÎìúÌïòÏÑ∏Ïöî.\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: RESULTS SUMMARY AND INTERPRETATION GUIDE\n",
        "# ============================================================================\n",
        "print(\"\\nüéØ Í≤∞Í≥º ÏöîÏïΩ Î∞è Ìï¥ÏÑù Í∞ÄÏù¥Îìú:\")\n",
        "print(\"=\"*60)\n",
        "print(\"üìã ÌÉêÏßÄ Í≤∞Í≥º ÏÉâÏÉÅ Íµ¨Î∂Ñ:\")\n",
        "print(\"üîµ ÌååÎûÄÏÉâ Î∞ïÏä§: Í∏∞Î≥∏ YOLO Î™®Îç∏ Í≤∞Í≥º\")\n",
        "print(\"   ‚îî‚îÄ‚îÄ ÌÉêÏßÄ Í∞ÄÎä•: ÏÇ¨Îûå, ÏûêÎèôÏ∞®, Ìä∏Îü≠, Î≤ÑÏä§, Ïò§ÌÜ†Î∞îÏù¥, ÏûêÏ†ÑÍ±∞, Ïã†Ìò∏Îì±, Ï†ïÏßÄÌëúÏßÄÌåê Îì±\")\n",
        "print(\"   ‚îî‚îÄ‚îÄ Ï¥ù 80Í∞ú ÌÅ¥ÎûòÏä§Ïùò ÏùºÎ∞òÏ†ÅÏù∏ Í∞ùÏ≤¥Îì§\")\n",
        "print(\"üî¥ Îπ®Í∞ÑÏÉâ Î∞ïÏä§: Ïª§Ïä§ÌÖÄ YOLO Î™®Îç∏ Í≤∞Í≥º\")\n",
        "print(\"   ‚îî‚îÄ‚îÄ ÌÉêÏßÄ Í∞ÄÎä•: Ï∞®ÏÑ†(lane), ÍµêÌÜµÌëúÏßÄÌåê(traffic_sign)\")\n",
        "print(\"   ‚îî‚îÄ‚îÄ ÎèÑÎ°ú ÌôòÍ≤Ω ÌäπÌôî Í∞ùÏ≤¥Îì§\")\n",
        "\n",
        "print(\"\\nüìä ÏÑ±Îä• ÌäπÏßï:\")\n",
        "print(\"‚Ä¢ Í∏∞Î≥∏ Î™®Îç∏: ÎÑìÏùÄ Î≤îÏúÑÏùò Í∞ùÏ≤¥ ÌÉêÏßÄ, ÏùºÎ∞òÏ†ÅÏù∏ ÏÉÅÌô©Ïóê Í∞ïÌï®\")\n",
        "print(\"‚Ä¢ Ïª§Ïä§ÌÖÄ Î™®Îç∏: ÌäπÏ†ï ÎèÑÎ©îÏù∏ ÌäπÌôî, Ï∞®ÏÑ†/ÌëúÏßÄÌåê ÌÉêÏßÄÏóê ÎÜíÏùÄ Ï†ïÌôïÎèÑ\")\n",
        "print(\"‚Ä¢ Í≤∞Ìï© Î™®Îç∏: Îëê Î™®Îç∏Ïùò Ïû•Ï†êÏùÑ Î™®Îëê ÌôúÏö©, Ìè¨Í¥ÑÏ†ÅÏù∏ Í∞ùÏ≤¥ ÌÉêÏßÄ\")\n",
        "\n",
        "print(\"\\nüíæ Îã§Ïö¥Î°úÎìú Í∞ÄÎä•Ìïú Í≤∞Í≥º ÌååÏùºÎì§:\")\n",
        "print(\"üìÅ /content/ Ìè¥ÎçîÏóê Ï†ÄÏû•Îêú ÌååÏùºÎì§:\")\n",
        "print(\"‚îú‚îÄ‚îÄ custom_result.mp4        : Ïª§Ïä§ÌÖÄ Î™®Îç∏ÎßåÏùò ÌÉêÏßÄ Í≤∞Í≥º\")\n",
        "print(\"‚îú‚îÄ‚îÄ base_result.mp4          : Í∏∞Î≥∏ Î™®Îç∏ÎßåÏùò ÌÉêÏßÄ Í≤∞Í≥º\")\n",
        "print(\"‚îú‚îÄ‚îÄ final_combined_result.mp4: Îëê Î™®Îç∏ Í≤∞Ìï© ÏµúÏ¢Ö Í≤∞Í≥º\")\n",
        "print(\"‚îî‚îÄ‚îÄ ÏõêÎ≥∏ ÏóÖÎ°úÎìú ÌååÏùºÎèÑ Ìï®Íªò Î≥¥Í¥ÄÎê®\")\n",
        "\n",
        "print(\"\\nüöÄ ÏÇ¨Ïö© Í∂åÏû•ÏÇ¨Ìï≠:\")\n",
        "print(\"‚Ä¢ ÏùºÎ∞òÏ†ÅÏù∏ Í∞ùÏ≤¥ ÌÉêÏßÄ: base_result.mp4 ÏÇ¨Ïö©\")\n",
        "print(\"‚Ä¢ ÎèÑÎ°ú/ÍµêÌÜµ ÌôòÍ≤Ω Î∂ÑÏÑù: custom_result.mp4 ÏÇ¨Ïö©\")\n",
        "print(\"‚Ä¢ Ï¢ÖÌï©Ï†ÅÏù∏ Î∂ÑÏÑù: final_combined_result.mp4 ÏÇ¨Ïö©\")\n",
        "\n",
        "print(\"\\n‚ú® Ï≤òÎ¶¨ ÏôÑÎ£å! Í≤∞Í≥º ÌååÏùºÎì§ÏùÑ Îã§Ïö¥Î°úÎìúÌïòÏó¨ ÌôïÏù∏ÌïòÏÑ∏Ïöî.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zO8V1XsmfpMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Compare inference speed between PyTorch and TensorRT models(with CPU)"
      ],
      "metadata": {
        "id": "l1QEksYFSalJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install ultralytics yt-dlp\n",
        "\n",
        "# Import all necessary libraries\n",
        "from ultralytics import YOLO           # YOLO model for object detection\n",
        "import glob                            # File path pattern matching\n",
        "import cv2                             # OpenCV for video processing\n",
        "import numpy as np                     # Numerical operations\n",
        "from IPython.display import Video      # Display videos in Jupyter\n",
        "import shutil                          # File operations\n",
        "import time                            # Time measurement for performance testing\n",
        "from google.colab import files         # File upload functionality for Google Colab\n",
        "\n",
        "# Create and save the dataset configuration file\n",
        "# This YAML file defines the dataset structure for custom model training/validation\n",
        "yaml_fix = '''path: /content/dataset\n",
        "train: train/images\n",
        "val: valid/images\n",
        "names:\n",
        "  0: lane\n",
        "  1: traffic_sign\n",
        "nc: 2'''\n",
        "\n",
        "# Write the configuration to a file\n",
        "with open('/content/dataset/dataset_fixed.yaml', 'w') as f:\n",
        "    f.write(yaml_fix)\n",
        "\n",
        "print(\"üöÄ TensorRT ÏµúÏ†ÅÌôî YOLO Ï∂îÎ°† ÏãúÏûë!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD BASE MODELS\n",
        "# ============================================================================\n",
        "print(\"ü§ñ Í∏∞Î≥∏ Î™®Îç∏ Î°úÎìú Ï§ë...\")\n",
        "print(\"   - Í∏∞Î≥∏ YOLO11n Î™®Îç∏: ÏùºÎ∞òÏ†ÅÏù∏ Í∞ùÏ≤¥ ÌÉêÏßÄÏö©\")\n",
        "print(\"   - Ïª§Ïä§ÌÖÄ Î™®Îç∏: Ï∞®ÏÑ† Î∞è ÍµêÌÜµÌëúÏßÄÌåê Ï†ÑÏö© ÌïôÏäµ Î™®Îç∏\")\n",
        "\n",
        "# Load the base YOLO11n model (pre-trained on COCO dataset)\n",
        "base_model = YOLO('yolo11n.pt')\n",
        "\n",
        "# Load your custom trained model for lane and traffic sign detection\n",
        "custom_model = YOLO('/content/dataset/best.pt')\n",
        "\n",
        "print(f\"üìã Í∏∞Î≥∏ Î™®Îç∏ ÌÅ¥ÎûòÏä§ Ïàò: {len(base_model.names)} (COCO Îç∞Ïù¥ÌÑ∞ÏÖã Í∏∞Î∞ò)\")\n",
        "print(f\"üìã Ïª§Ïä§ÌÖÄ Î™®Îç∏ ÌÅ¥ÎûòÏä§ Ïàò: {len(custom_model.names)} (lane, traffic_sign)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: CHECK GPU AVAILABILITY AND MODEL OPTIMIZATION\n",
        "# ============================================================================\n",
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "cuda_available = torch.cuda.is_available()\n",
        "device_count = torch.cuda.device_count()\n",
        "\n",
        "print(f\"\\nüîç ÏãúÏä§ÌÖú ÌôòÍ≤Ω ÌôïÏù∏:\")\n",
        "print(f\"   CUDA ÏÇ¨Ïö© Í∞ÄÎä•: {cuda_available}\")\n",
        "print(f\"   GPU Í∞úÏàò: {device_count}\")\n",
        "\n",
        "if cuda_available:\n",
        "    print(f\"   GPU Ïû•Ïπò: {torch.cuda.get_device_name(0)}\")\n",
        "    use_tensorrt = True\n",
        "    device = 0\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è GPUÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§. CPU Î™®ÎìúÎ°ú Ïã§ÌñâÎê©ÎãàÎã§.\")\n",
        "    use_tensorrt = False\n",
        "    device = 'cpu'\n",
        "\n",
        "if use_tensorrt:\n",
        "    # ============================================================================\n",
        "    # STEP 2A: CONVERT MODELS TO TENSORRT FORMAT (GPU ONLY)\n",
        "    # ============================================================================\n",
        "    print(\"\\n‚ö° TensorRT Î≥ÄÌôò Ï§ë...\")\n",
        "    print(\"   TensorRTÎäî NVIDIA GPUÏóêÏÑú Ï∂îÎ°† ÏÜçÎèÑÎ•º ÌÅ¨Í≤å Ìñ•ÏÉÅÏãúÌÇ§Îäî ÏµúÏ†ÅÌôî ÏóîÏßÑÏûÖÎãàÎã§\")\n",
        "\n",
        "    try:\n",
        "        # Convert base model to TensorRT format with FP16 precision for speed optimization\n",
        "        print(\"üîÑ Í∏∞Î≥∏ Î™®Îç∏ ‚Üí TensorRT Î≥ÄÌôò Ï§ë... (FP16 Ï†ïÎ∞ÄÎèÑÎ°ú ÏµúÏ†ÅÌôî)\")\n",
        "        base_model.export(format='engine', half=True, device=device)\n",
        "        base_trt_path = 'yolo11n.engine'\n",
        "\n",
        "        # Convert custom model to TensorRT format\n",
        "        print(\"üîÑ Ïª§Ïä§ÌÖÄ Î™®Îç∏ ‚Üí TensorRT Î≥ÄÌôò Ï§ë... (FP16 Ï†ïÎ∞ÄÎèÑÎ°ú ÏµúÏ†ÅÌôî)\")\n",
        "        custom_model.export(format='engine', half=True, device=device)\n",
        "        custom_trt_path = '/content/dataset/best.engine'\n",
        "\n",
        "        # Load TensorRT optimized models\n",
        "        print(\"\\nüî• TensorRT ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ Î°úÎìú Ï§ë...\")\n",
        "        print(\"   ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ÏùÄ ÏùºÎ∞ò PyTorch Î™®Îç∏Î≥¥Îã§ 2-5Î∞∞ Îπ†Î•∏ Ï∂îÎ°† ÏÜçÎèÑÎ•º Ï†úÍ≥µÌï©ÎãàÎã§\")\n",
        "\n",
        "        base_trt_model = YOLO(base_trt_path)      # TensorRT optimized base model\n",
        "        custom_trt_model = YOLO(custom_trt_path)  # TensorRT optimized custom model\n",
        "\n",
        "        print(\"‚úÖ TensorRT Î™®Îç∏ Î°úÎìú ÏôÑÎ£å!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå TensorRT Î≥ÄÌôò Ïã§Ìå®: {e}\")\n",
        "        print(\"üîÑ CPU Î™®ÎìúÎ°ú Ï†ÑÌôòÌï©ÎãàÎã§...\")\n",
        "        use_tensorrt = False\n",
        "\n",
        "if not use_tensorrt:\n",
        "    # ============================================================================\n",
        "    # STEP 2B: USE PYTORCH MODELS ON CPU\n",
        "    # ============================================================================\n",
        "    print(\"\\nüêç PyTorch CPU Î™®ÎìúÎ°ú Ïã§Ìñâ\")\n",
        "    print(\"   GPUÍ∞Ä ÏóÜÍ±∞ÎÇò TensorRT Î≥ÄÌôòÏóê Ïã§Ìå®ÌïòÏó¨ CPUÏóêÏÑú PyTorch Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§\")\n",
        "\n",
        "    # Use original PyTorch models\n",
        "    base_trt_model = base_model      # Use original base model\n",
        "    custom_trt_model = custom_model  # Use original custom model\n",
        "\n",
        "    print(\"‚úÖ PyTorch CPU Î™®Îç∏ Ï§ÄÎπÑ ÏôÑÎ£å!\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: UPLOAD VIDEO FILES FROM PC\n",
        "# ============================================================================\n",
        "print(\"\\nüì• ÎπÑÎîîÏò§ ÌååÏùº ÏóÖÎ°úÎìú\")\n",
        "print(\"   Î∏åÎùºÏö∞Ï†Ä Îã§Ïù¥ÏñºÎ°úÍ∑∏Î•º ÌÜµÌï¥ PCÏóêÏÑú ÎπÑÎîîÏò§ ÌååÏùºÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî\")\n",
        "\n",
        "# Open file upload dialog for users to select video files from their PC\n",
        "print(\"\\nüé¨ ÏóÖÎ°úÎìúÌï† ÎπÑÎîîÏò§ ÌååÏùºÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:\")\n",
        "uploaded = files.upload()  # Returns a dictionary: {filename: file_content}\n",
        "\n",
        "# Check if any files were actually uploaded\n",
        "if not uploaded:\n",
        "    print(\"‚ùå ÏóÖÎ°úÎìúÎêú ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§!\")\n",
        "    raise Exception(\"No files uploaded\")  # Stop execution if no files uploaded\n",
        "\n",
        "print(f\"\\n‚úÖ {len(uploaded)} Í∞úÏùò ÌååÏùºÏù¥ ÏóÖÎ°úÎìúÎêòÏóàÏäµÎãàÎã§\")\n",
        "\n",
        "# Get the first uploaded video file path\n",
        "# The uploaded files are automatically saved to the current directory\n",
        "uploaded_filenames = list(uploaded.keys())\n",
        "video_path = uploaded_filenames[0]  # Use the first uploaded file\n",
        "print(f\"üìπ Ï≤òÎ¶¨Ìï† ÎπÑÎîîÏò§: {video_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: PERFORMANCE COMPARISON FUNCTION\n",
        "# ============================================================================\n",
        "def performance_comparison(video_path, frames_to_test=100):\n",
        "    \"\"\"\n",
        "    Compare inference speed between standard and optimized models\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to the video file for testing\n",
        "        frames_to_test: Number of frames to use for speed comparison\n",
        "\n",
        "    Returns:\n",
        "        speedup_ratio: How many times faster optimized models are\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n‚è±Ô∏è ÏÑ±Îä• ÎπÑÍµê ÌÖåÏä§Ìä∏ (Ï≤´ {frames_to_test}ÌîÑÎ†àÏûÑÏúºÎ°ú Ï∏°Ï†ï)\")\n",
        "    if use_tensorrt:\n",
        "        print(\"   PyTorch vs TensorRT Ï∂îÎ°† ÏÜçÎèÑÎ•º Ï†ïÌôïÌûà ÎπÑÍµêÌï©ÎãàÎã§\")\n",
        "    else:\n",
        "        print(\"   ÌëúÏ§Ä PyTorch vs ÏµúÏ†ÅÌôîÎêú PyTorch Ï∂îÎ°† ÏÜçÎèÑÎ•º ÎπÑÍµêÌï©ÎãàÎã§\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Open video capture object for reading frames\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # ========== Standard Models Performance Test ==========\n",
        "    print(\"üêç ÌëúÏ§Ä Î™®Îç∏ ÏÑ±Îä• Ï∏°Ï†ï Ï§ë...\")\n",
        "    standard_times = []  # Store processing time for each frame\n",
        "\n",
        "    for i in range(frames_to_test):\n",
        "        ret, frame = cap.read()  # Read next frame\n",
        "        if not ret:  # End of video\n",
        "            break\n",
        "\n",
        "        # Measure inference time for both models\n",
        "        start_time = time.time()\n",
        "        _ = base_model(frame, verbose=False)      # Base model inference\n",
        "        _ = custom_model(frame, verbose=False)    # Custom model inference\n",
        "        end_time = time.time()\n",
        "\n",
        "        standard_times.append(end_time - start_time)\n",
        "\n",
        "    # ========== Optimized Models Performance Test ==========\n",
        "    if use_tensorrt:\n",
        "        print(\"‚ö° TensorRT Î™®Îç∏ ÏÑ±Îä• Ï∏°Ï†ï Ï§ë...\")\n",
        "    else:\n",
        "        print(\"üîß ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ ÏÑ±Îä• Ï∏°Ï†ï Ï§ë...\")\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset video to beginning\n",
        "    optimized_times = []  # Store processing time for each frame\n",
        "\n",
        "    for i in range(frames_to_test):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Measure inference time for both optimized models\n",
        "        start_time = time.time()\n",
        "        _ = base_trt_model(frame, verbose=False)     # Optimized base model\n",
        "        _ = custom_trt_model(frame, verbose=False)   # Optimized custom model\n",
        "        end_time = time.time()\n",
        "\n",
        "        optimized_times.append(end_time - start_time)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # ========== Calculate and Display Results ==========\n",
        "    standard_avg = np.mean(standard_times) * 1000    # Convert to milliseconds\n",
        "    optimized_avg = np.mean(optimized_times) * 1000\n",
        "    speedup = standard_avg / optimized_avg           # Calculate speedup ratio\n",
        "\n",
        "    model_type = \"TensorRT\" if use_tensorrt else \"ÏµúÏ†ÅÌôîÎêú PyTorch\"\n",
        "\n",
        "    print(f\"üêç ÌëúÏ§Ä PyTorch ÌèâÍ∑†: {standard_avg:.2f}ms/frame ({1000/standard_avg:.1f} FPS)\")\n",
        "    print(f\"‚ö° {model_type} ÌèâÍ∑†: {optimized_avg:.2f}ms/frame ({1000/optimized_avg:.1f} FPS)\")\n",
        "    print(f\"üöÄ ÏÜçÎèÑ Ìñ•ÏÉÅ: {speedup:.2f}x Îπ®ÎùºÏßê\")\n",
        "\n",
        "    return speedup\n",
        "\n",
        "# Execute performance comparison\n",
        "speedup_ratio = performance_comparison(video_path)\n",
        "\n",
        "# Set model type for display purposes\n",
        "model_type = \"TensorRT\" if use_tensorrt else \"ÏµúÏ†ÅÌôîÎêú\"\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: OPTIMIZED COMBINED INFERENCE\n",
        "# ============================================================================\n",
        "def optimized_combined_inference(video_path, output_path='/content/optimized_result.mp4'):\n",
        "    \"\"\"\n",
        "    Process entire video using optimized models with visual output\n",
        "\n",
        "    Args:\n",
        "        video_path: Input video file path\n",
        "        output_path: Output processed video file path\n",
        "\n",
        "    Returns:\n",
        "        avg_fps: Average processing speed in frames per second\n",
        "    \"\"\"\n",
        "\n",
        "    # ========== Video Properties Setup ==========\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))           # Original video FPS\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Video width\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Video height\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Total frame count\n",
        "\n",
        "    # Setup output video writer with same properties as input\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Video codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    model_type = \"TensorRT\" if use_tensorrt else \"ÏµúÏ†ÅÌôîÎêú\"\n",
        "    print(f\"\\nüé¨ {model_type} ÏòÅÏÉÅ Ï≤òÎ¶¨ ÏãúÏûë...\")\n",
        "    print(f\"   üìπ Ìï¥ÏÉÅÎèÑ: {width}x{height}\")\n",
        "    print(f\"   üéûÔ∏è FPS: {fps}\")\n",
        "    print(f\"   üìä Ï¥ù ÌîÑÎ†àÏûÑ: {total_frames}\")\n",
        "    print(f\"   üñ•Ô∏è Ï≤òÎ¶¨ Ïû•Ïπò: {'GPU (TensorRT)' if use_tensorrt else 'CPU (PyTorch)'}\")\n",
        "\n",
        "    # ========== Frame-by-Frame Processing ==========\n",
        "    frame_count = 0\n",
        "    total_inference_time = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:  # End of video\n",
        "            break\n",
        "\n",
        "        # ========== Optimized Inference (with timing) ==========\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run inference on both optimized models\n",
        "        base_results = base_trt_model(frame, verbose=False)     # General objects\n",
        "        custom_results = custom_trt_model(frame, verbose=False) # Lanes & traffic signs\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "        total_inference_time += inference_time\n",
        "\n",
        "        # ========== Visualization of Detection Results ==========\n",
        "        annotated_frame = frame.copy()\n",
        "\n",
        "        # Draw base YOLO detection results (BLUE boxes)\n",
        "        if base_results[0].boxes is not None:\n",
        "            for box in base_results[0].boxes:\n",
        "                # Extract bounding box coordinates\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])    # Confidence score\n",
        "                cls = int(box.cls[0])        # Class index\n",
        "\n",
        "                # Only draw boxes with confidence > 0.3\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{base_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    # Draw blue rectangle and text\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # Draw custom YOLO detection results (RED boxes)\n",
        "        if custom_results[0].boxes is not None:\n",
        "            for box in custom_results[0].boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{custom_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    # Draw red rectangle and text\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # Add real-time FPS information (GREEN text)\n",
        "        fps_text = f\"{model_type}: {1/inference_time:.1f} FPS\"\n",
        "        cv2.putText(annotated_frame, fps_text, (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Write processed frame to output video\n",
        "        out.write(annotated_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "        # Progress reporting every 50 frames\n",
        "        if frame_count % 50 == 0:\n",
        "            avg_fps = frame_count / total_inference_time\n",
        "            progress = frame_count / total_frames * 100\n",
        "            print(f\"   üîÑ Ï≤òÎ¶¨ Ï§ë... {frame_count}/{total_frames} ({progress:.1f}%) - ÌèâÍ∑† {avg_fps:.1f} FPS\")\n",
        "\n",
        "    # ========== Cleanup and Results ==========\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    avg_fps = frame_count / total_inference_time\n",
        "    print(f\"‚úÖ {model_type} Í≤∞Í≥º ÏòÅÏÉÅ Ï†ÄÏû• ÏôÑÎ£å: {output_path}\")\n",
        "    print(f\"üìä ÏµúÏ¢Ö ÌèâÍ∑† Ï≤òÎ¶¨ ÏÜçÎèÑ: {avg_fps:.1f} FPS\")\n",
        "\n",
        "    return avg_fps\n",
        "    \"\"\"\n",
        "    Process entire video using TensorRT optimized models with visual output\n",
        "\n",
        "    Args:\n",
        "        video_path: Input video file path\n",
        "        output_path: Output processed video file path\n",
        "\n",
        "    Returns:\n",
        "        avg_fps: Average processing speed in frames per second\n",
        "    \"\"\"\n",
        "\n",
        "    # ========== Video Properties Setup ==========\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))           # Original video FPS\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Video width\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Video height\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Total frame count\n",
        "\n",
        "    # Setup output video writer with same properties as input\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Video codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    print(f\"\\nüé¨ TensorRT ÏµúÏ†ÅÌôî ÏòÅÏÉÅ Ï≤òÎ¶¨ ÏãúÏûë...\")\n",
        "    print(f\"   üìπ Ìï¥ÏÉÅÎèÑ: {width}x{height}\")\n",
        "    print(f\"   üéûÔ∏è FPS: {fps}\")\n",
        "    print(f\"   üìä Ï¥ù ÌîÑÎ†àÏûÑ: {total_frames}\")\n",
        "\n",
        "    # ========== Frame-by-Frame Processing ==========\n",
        "    frame_count = 0\n",
        "    total_inference_time = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:  # End of video\n",
        "            break\n",
        "\n",
        "        # ========== TensorRT Inference (with timing) ==========\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run inference on both TensorRT optimized models\n",
        "        base_results = base_trt_model(frame, verbose=False)     # General objects\n",
        "        custom_results = custom_trt_model(frame, verbose=False) # Lanes & traffic signs\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "        total_inference_time += inference_time\n",
        "\n",
        "        # ========== Visualization of Detection Results ==========\n",
        "        annotated_frame = frame.copy()\n",
        "\n",
        "        # Draw base YOLO detection results (BLUE boxes)\n",
        "        if base_results[0].boxes is not None:\n",
        "            for box in base_results[0].boxes:\n",
        "                # Extract bounding box coordinates\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])    # Confidence score\n",
        "                cls = int(box.cls[0])        # Class index\n",
        "\n",
        "                # Only draw boxes with confidence > 0.3\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{base_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    # Draw blue rectangle and text\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # Draw custom YOLO detection results (RED boxes)\n",
        "        if custom_results[0].boxes is not None:\n",
        "            for box in custom_results[0].boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{custom_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    # Draw red rectangle and text\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # Add real-time FPS information (GREEN text)\n",
        "        fps_text = f\"TensorRT: {1/inference_time:.1f} FPS\"\n",
        "        cv2.putText(annotated_frame, fps_text, (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Write processed frame to output video\n",
        "        out.write(annotated_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "        # Progress reporting every 50 frames\n",
        "        if frame_count % 50 == 0:\n",
        "            avg_fps = frame_count / total_inference_time\n",
        "            progress = frame_count / total_frames * 100\n",
        "            print(f\"   üîÑ Ï≤òÎ¶¨ Ï§ë... {frame_count}/{total_frames} ({progress:.1f}%) - ÌèâÍ∑† {avg_fps:.1f} FPS\")\n",
        "\n",
        "    # ========== Cleanup and Results ==========\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    avg_fps = frame_count / total_inference_time\n",
        "    print(f\"‚úÖ TensorRT Í≤∞Í≥º ÏòÅÏÉÅ Ï†ÄÏû• ÏôÑÎ£å: {output_path}\")\n",
        "    print(f\"üìä ÏµúÏ¢Ö ÌèâÍ∑† Ï≤òÎ¶¨ ÏÜçÎèÑ: {avg_fps:.1f} FPS\")\n",
        "\n",
        "    return avg_fps\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: EXECUTE OPTIMIZED INFERENCE\n",
        "# ============================================================================\n",
        "print(f\"\\nüî• {model_type} ÏµúÏ†ÅÌôîÎêú Í≤∞Ìï© Ï∂îÎ°† Ïã§Ìñâ...\")\n",
        "if use_tensorrt:\n",
        "    print(\"   Ï†ÑÏ≤¥ ÎπÑÎîîÏò§Î•º TensorRTÎ°ú ÏµúÏ†ÅÌôîÎêú Îëê Î™®Îç∏Î°ú Ï≤òÎ¶¨Ìï©ÎãàÎã§\")\n",
        "    output_filename = '/content/tensorrt_final_result.mp4'\n",
        "    tensorrt_fps = optimized_combined_inference(video_path, output_filename) # Assign returned value\n",
        "else:\n",
        "    print(\"   Ï†ÑÏ≤¥ ÎπÑÎîîÏò§Î•º CPUÏóêÏÑú ÏµúÏ†ÅÌôîÎêú Îëê Î™®Îç∏Î°ú Ï≤òÎ¶¨Ìï©ÎãàÎã§\")\n",
        "    output_filename = '/content/cpu_final_result.mp4'\n",
        "    # When not using TensorRT, optimized_combined_inference uses the \"optimized\" PyTorch models.\n",
        "    # We will assign its result to optimized_fps as it represents the performance of the optimized path.\n",
        "    optimized_fps = optimized_combined_inference(video_path, output_filename)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: STANDARD PYTORCH INFERENCE FOR COMPARISON\n",
        "# ============================================================================\n",
        "print(\"\\nüêç ÌëúÏ§Ä PyTorch Ï∂îÎ°† (ÎπÑÍµê Í∏∞Ï§ÄÏö©)...\")\n",
        "print(\"   ÏÑ±Îä• ÎπÑÍµêÎ•º ÏúÑÌï¥ ÎèôÏùºÌïú ÏòÅÏÉÅÏùÑ ÌëúÏ§Ä PyTorch Î™®Îç∏Î°ú Ï≤òÎ¶¨Ìï©ÎãàÎã§\")\n",
        "\n",
        "def standard_pytorch_inference(video_path, output_path='/content/standard_pytorch_result.mp4'):\n",
        "    \"\"\"\n",
        "    Process video using standard PyTorch models for performance comparison\n",
        "\n",
        "    Args:\n",
        "        video_path: Input video file path\n",
        "        output_path: Output video file path\n",
        "\n",
        "    Returns:\n",
        "        standard_fps: Average processing speed in FPS\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Setup video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Run inference with standard PyTorch models\n",
        "        base_results = base_model(frame, verbose=False)\n",
        "        custom_results = custom_model(frame, verbose=False)\n",
        "\n",
        "        # Simple annotation for comparison (without detailed boxes)\n",
        "        annotated_frame = frame.copy()\n",
        "        cv2.putText(annotated_frame, \"Standard PyTorch\", (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
        "\n",
        "        out.write(annotated_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "        # Process only first 100 frames for quick comparison\n",
        "        if frame_count >= 100:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    standard_fps = frame_count / total_time\n",
        "    return standard_fps\n",
        "\n",
        "# Execute standard PyTorch inference for comparison\n",
        "# Assign the returned value to pytorch_fps\n",
        "pytorch_fps = standard_pytorch_inference(video_path, output_path='/content/pytorch_result.mp4')\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: MODEL PERFORMANCE EVALUATION\n",
        "# ============================================================================\n",
        "print(\"\\nüìä Ïª§Ïä§ÌÖÄ Î™®Îç∏ Ï†ïÌôïÎèÑ ÌèâÍ∞Ä:\")\n",
        "print(\"   Validation Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏Ïùò Ï†ïÌôïÎèÑÎ•º Ï∏°Ï†ïÌï©ÎãàÎã§\")\n",
        "\n",
        "# Evaluate custom model performance on validation dataset\n",
        "metrics = custom_model.val(data='/content/dataset/dataset_fixed.yaml')\n",
        "print(f\"üéØ mAP50 (Mean Average Precision): {metrics.box.map50:.4f}\")\n",
        "print(\"   mAP50ÏùÄ IoU 0.5ÏóêÏÑúÏùò ÌèâÍ∑† Ï†ïÎ∞ÄÎèÑÎ°ú, ÎÜíÏùÑÏàòÎ°ù Ï¢ãÏäµÎãàÎã§ (ÏµúÎåÄÍ∞í: 1.0)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: FINAL RESULTS AND COMPARISON\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ ÏµúÏ¢Ö ÏÑ±Îä• ÎπÑÍµê Í≤∞Í≥º:\")\n",
        "print(\"-\" * 30)\n",
        "# Use the correct variables based on whether TensorRT was used\n",
        "if use_tensorrt:\n",
        "    print(f\"üêç PyTorch Ï≤òÎ¶¨ ÏÜçÎèÑ:  {pytorch_fps:.1f} FPS\")\n",
        "    print(f\"‚ö° TensorRT Ï≤òÎ¶¨ ÏÜçÎèÑ:  {tensorrt_fps:.1f} FPS\")\n",
        "    print(f\"üöÄ Ï†ÑÏ≤¥ ÏÜçÎèÑ Ìñ•ÏÉÅ ÎπÑÏú®: {tensorrt_fps/pytorch_fps:.2f}x Îπ®ÎùºÏßê\")\n",
        "else:\n",
        "    # If TensorRT was not used, compare standard PyTorch with the \"optimized\" PyTorch run\n",
        "    print(f\"üêç ÌëúÏ§Ä PyTorch Ï≤òÎ¶¨ ÏÜçÎèÑ:  {pytorch_fps:.1f} FPS\")\n",
        "    print(f\"üîß ÏµúÏ†ÅÌôîÎêú PyTorch Ï≤òÎ¶¨ ÏÜçÎèÑ:  {optimized_fps:.1f} FPS\")\n",
        "    print(f\"üöÄ Ï†ÑÏ≤¥ ÏÜçÎèÑ Ìñ•ÏÉÅ ÎπÑÏú®: {optimized_fps/pytorch_fps:.2f}x Îπ®ÎùºÏßê\")\n",
        "\n",
        "\n",
        "print(f\"üìä Î™®Îç∏ Ï†ïÌôïÎèÑ (mAP50): {metrics.box.map50:.4f}\")\n",
        "\n",
        "print(\"\\nüé¨ ÏµúÏ¢Ö TensorRT ÏµúÏ†ÅÌôî Í≤∞Í≥º ÏòÅÏÉÅ:\")\n",
        "# Display the correct output video based on whether TensorRT was used\n",
        "if use_tensorrt:\n",
        "    Video('/content/tensorrt_final_result.mp4', width=800)\n",
        "else:\n",
        "    Video('/content/cpu_final_result.mp4', width=800)\n",
        "\n",
        "\n",
        "print(\"\\nüéâ ÏµúÏ†ÅÌôî ÏôÑÎ£å!\")\n",
        "print(\"=\"*60)\n",
        "print(\"üìã Í≤∞Í≥º Ìï¥ÏÑù Í∞ÄÏù¥Îìú:\")\n",
        "print(\"üîµ ÌååÎûÄÏÉâ Î∞ïÏä§: Í∏∞Î≥∏ YOLOÍ∞Ä ÌÉêÏßÄÌïú ÏùºÎ∞ò Í∞ùÏ≤¥Îì§ (TensorRT ÏµúÏ†ÅÌôî)\")\n",
        "print(\"üî¥ Îπ®Í∞ÑÏÉâ Î∞ïÏä§: Ïª§Ïä§ÌÖÄ Î™®Îç∏Ïù¥ ÌÉêÏßÄÌïú Ï∞®ÏÑ†/ÍµêÌÜµÌëúÏßÄÌåê (TensorRT ÏµúÏ†ÅÌôî)\")\n",
        "print(\"üíö Ï¥àÎ°ùÏÉâ ÌÖçÏä§Ìä∏: Ïã§ÏãúÍ∞Ñ FPS ÌëúÏãú (Ï≤òÎ¶¨ ÏÜçÎèÑ Î™®ÎãàÌÑ∞ÎßÅ)\")\n",
        "\n",
        "print(\"\\nüíæ ÏÉùÏÑ±Îêú ÌååÏùºÎì§:\")\n",
        "if use_tensorrt:\n",
        "    print(\"- tensorrt_final_result.mp4: TensorRT ÏµúÏ†ÅÌôîÎêú ÏµúÏ¢Ö Í≤∞Í≥º ÏòÅÏÉÅ\")\n",
        "    print(\"- yolo11n.engine: Í∏∞Î≥∏ YOLO Î™®Îç∏Ïùò TensorRT ÏóîÏßÑ ÌååÏùº\")\n",
        "    print(\"- best.engine: Ïª§Ïä§ÌÖÄ Î™®Îç∏Ïùò TensorRT ÏóîÏßÑ ÌååÏùº\")\n",
        "else:\n",
        "    print(\"- cpu_final_result.mp4: CPUÏóêÏÑú ÏµúÏ†ÅÌôîÎêú ÏµúÏ¢Ö Í≤∞Í≥º ÏòÅÏÉÅ\")\n",
        "print(\"- pytorch_result.mp4: PyTorch Í∏∞Î≥∏ Î™®Îç∏ ÎπÑÍµêÏö© ÏòÅÏÉÅ\")\n",
        "\n",
        "\n",
        "print(\"\\nüöÄ ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏôÑÎ£å!\")\n",
        "# Use the correct variables based on whether TensorRT was used\n",
        "if use_tensorrt:\n",
        "    print(f\"   ÏµúÏ¢Ö ÏÜçÎèÑ Ìñ•ÏÉÅ: {speedup_ratio:.1f}x (Í∞úÎ≥Ñ ÌîÑÎ†àÏûÑ Í∏∞Ï§Ä)\")\n",
        "    print(f\"   Ï†ÑÏ≤¥ ÏòÅÏÉÅ Ï≤òÎ¶¨: {tensorrt_fps/pytorch_fps:.1f}x (Ï†ÑÏ≤¥ ÏòÅÏÉÅ Í∏∞Ï§Ä)\")\n",
        "else:\n",
        "    print(f\"   ÏµúÏ¢Ö ÏÜçÎèÑ Ìñ•ÏÉÅ: {speedup_ratio:.1f}x (Í∞úÎ≥Ñ ÌîÑÎ†àÏûÑ Í∏∞Ï§Ä)\")\n",
        "    print(f\"   Ï†ÑÏ≤¥ ÏòÅÏÉÅ Ï≤òÎ¶¨: {optimized_fps/pytorch_fps:.1f}x (Ï†ÑÏ≤¥ ÏòÅÏÉÅ Í∏∞Ï§Ä)\")"
      ],
      "metadata": {
        "id": "JF5702JNN-Hv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}