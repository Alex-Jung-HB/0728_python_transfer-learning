{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlp09u4RXn/SBv7o8hjNWW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex-Jung-HB/0728_python_transfer-learning/blob/main/0728_python_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Mount a google drive"
      ],
      "metadata": {
        "id": "LfxjF8pQePNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF4m5_eZykW-",
        "outputId": "71ade7d9-98b6-470a-bb0e-3e84dfe50a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File copy to Colab"
      ],
      "metadata": {
        "id": "UEkGfYBbeXo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ZIP íŒŒì¼ì„ ì½”ë©ìœ¼ë¡œ ë³µì‚¬\n",
        "!cp \"/content/drive/MyDrive/dataset.zip\" \"/content/\"\n",
        "\n",
        "# ì••ì¶• í•´ì œ\n",
        "!unzip -o /content/dataset.zip -d /content/\n",
        "\n",
        "# ì••ì¶• í•´ì œ í™•ì¸\n",
        "!ls -la /content/"
      ],
      "metadata": {
        "id": "xneYgsP4z-XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "File copy to Colab"
      ],
      "metadata": {
        "id": "FuA9pjYYdQCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“š COMPLETE ANNOTATED GUIDE - ZIP FILE IN GOOGLE COLAB\n",
        "# Every command explained step by step for easy understanding\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ”— STEP 1: MOUNT GOOGLE DRIVE\n",
        "# =============================================================================\n",
        "\n",
        "# Import the drive module from google.colab\n",
        "# This module allows us to connect Colab to your Google Drive account\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount (connect) your Google Drive to Colab\n",
        "# This creates a bridge between Colab and your personal Google Drive\n",
        "# After this, your Drive files will be accessible at /content/drive/MyDrive/\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ğŸ“ What happens when you run this:\n",
        "# - A popup appears asking for permission to access your Google Drive\n",
        "# - You click \"Connect to Google Drive\"\n",
        "# - You sign in to your Google account\n",
        "# - You grant permission to Colab\n",
        "# - Your Drive becomes accessible in Colab at the path /content/drive/MyDrive/\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ” STEP 2: VERIFY THE FILE EXISTS (OPTIONAL BUT RECOMMENDED)\n",
        "# =============================================================================\n",
        "\n",
        "# List detailed information about your specific ZIP file\n",
        "# -l = long format (shows permissions, size, date, owner)\n",
        "# -a = show all files (including hidden ones starting with .)\n",
        "!ls -la \"/content/drive/MyDrive/dataset.zip\"\n",
        "\n",
        "# ğŸ“ What this command does:\n",
        "# - Checks if dataset.zip actually exists in your Google Drive\n",
        "# - Shows file size, creation date, and permissions\n",
        "# - If file doesn't exist, you'll get \"No such file or directory\" error\n",
        "# - If file exists, you'll see something like: -rw------- 1 root root 140123456 Jul 28 dataset.zip\n",
        "\n",
        "# Alternative: Check what's in your entire Drive root folder\n",
        "!ls -la /content/drive/MyDrive/\n",
        "\n",
        "# ğŸ“ This shows ALL files and folders in your Google Drive root\n",
        "# Helpful to see everything you have and find your ZIP file\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ“‚ STEP 3: COPY FILE FROM GOOGLE DRIVE TO COLAB WORKSPACE\n",
        "# =============================================================================\n",
        "\n",
        "# Copy the ZIP file from Google Drive to Colab's local storage\n",
        "# cp = copy command\n",
        "# \"/content/drive/MyDrive/dataset.zip\" = source (where to copy FROM)\n",
        "# /content/ = destination (where to copy TO)\n",
        "!cp \"/content/drive/MyDrive/dataset.zip\" /content/\n",
        "\n",
        "# ğŸ“ Why we use quotes around the source path:\n",
        "# - Protects against special characters or spaces in filenames\n",
        "# - Good practice even when not strictly necessary\n",
        "# - \"/content/drive/MyDrive/dataset.zip\" is safer than /content/drive/MyDrive/dataset.zip\n",
        "\n",
        "# ğŸ“ Understanding the paths:\n",
        "# - /content/drive/MyDrive/ = Your Google Drive (permanent storage)\n",
        "# - /content/ = Colab's workspace (temporary storage, deleted when session ends)\n",
        "# - We copy from permanent storage to temporary workspace for faster processing\n",
        "\n",
        "# =============================================================================\n",
        "# âœ… STEP 4: VERIFY THE COPY WAS SUCCESSFUL\n",
        "# =============================================================================\n",
        "\n",
        "# Check if the file was successfully copied to Colab's workspace\n",
        "!ls -la /content/dataset.zip\n",
        "\n",
        "# ğŸ“ What to expect:\n",
        "# - If successful: Shows file details like -rw-r--r-- 1 root root 140123456 Jul 28 dataset.zip\n",
        "# - If failed: \"No such file or directory\" error\n",
        "# - The file size should match what you saw in your Google Drive\n",
        "\n",
        "# Alternative: List all ZIP files in the workspace\n",
        "!ls -la /content/*.zip\n",
        "\n",
        "# ğŸ“ The asterisk (*) is a wildcard:\n",
        "# - *.zip means \"any filename ending with .zip\"\n",
        "# - Shows all ZIP files in the /content/ directory\n",
        "# - Useful if you have multiple ZIP files\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ“¦ STEP 5: EXTRACT (UNZIP) THE FILE\n",
        "# =============================================================================\n",
        "\n",
        "# Extract the contents of the ZIP file\n",
        "# unzip = command to extract ZIP archives\n",
        "# -o = overwrite existing files without asking for confirmation\n",
        "# /content/dataset.zip = the ZIP file to extract (source)\n",
        "# -d /content/ = destination directory where files will be extracted\n",
        "!unzip -o /content/dataset.zip -d /content/\n",
        "\n",
        "# ğŸ“ Breaking down the flags:\n",
        "# -o (overwrite): If files already exist, replace them without asking\n",
        "# -d (directory): Specifies where to extract the files\n",
        "# Without -d, files would extract to the current directory\n",
        "\n",
        "# ğŸ“ What happens during extraction:\n",
        "# - The ZIP file is opened and read\n",
        "# - All files/folders inside are created in /content/\n",
        "# - You'll see output like \"inflating: file1.txt\" for each extracted file\n",
        "# - The original ZIP file remains unchanged (it's not deleted)\n",
        "\n",
        "# Alternative extraction commands you might see:\n",
        "\n",
        "# Extract without overwriting (asks for confirmation if files exist)\n",
        "# !unzip /content/dataset.zip -d /content/\n",
        "\n",
        "# Just see what's inside the ZIP without extracting\n",
        "# !unzip -l /content/dataset.zip\n",
        "\n",
        "# Test the ZIP file integrity without extracting\n",
        "# !unzip -t /content/dataset.zip\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ“‹ STEP 6: CHECK WHAT WAS EXTRACTED\n",
        "# =============================================================================\n",
        "\n",
        "# List all contents of the workspace to see what was extracted\n",
        "!ls -la /content/\n",
        "\n",
        "# ğŸ“ What you'll see:\n",
        "# - All the files and folders that were inside your ZIP\n",
        "# - The original dataset.zip file (still there)\n",
        "# - sample_data/ folder (default Colab folder, ignore this)\n",
        "# - drive/ folder (your mounted Google Drive, ignore this)\n",
        "\n",
        "# More detailed view of extracted contents\n",
        "!ls -laR /content/\n",
        "\n",
        "# ğŸ“ The -R flag means \"recursive\":\n",
        "# - Shows contents of all subdirectories too\n",
        "# - Useful if your ZIP contained nested folders\n",
        "# - Can be overwhelming for large datasets\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ§¹ STEP 7: CLEAN UP (OPTIONAL BUT RECOMMENDED)\n",
        "# =============================================================================\n",
        "\n",
        "# Remove the ZIP file to save space (since we've extracted everything)\n",
        "# rm = remove (delete) command\n",
        "!rm /content/dataset.zip\n",
        "\n",
        "# ğŸ“ Why delete the ZIP file:\n",
        "# - Saves disk space (ZIP file is now redundant)\n",
        "# - Colab has limited storage space\n",
        "# - We have all the contents extracted, so ZIP file is no longer needed\n",
        "# - You can always re-copy from Google Drive if needed\n",
        "\n",
        "# ğŸ“ BE CAREFUL: rm permanently deletes files!\n",
        "# - No \"recycle bin\" in Linux/Colab\n",
        "# - Make sure you've successfully extracted before deleting\n",
        "# - The original ZIP is still safe in your Google Drive\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ” STEP 8: VERIFY EVERYTHING IS WORKING\n",
        "# =============================================================================\n",
        "\n",
        "# Final check - list everything to confirm setup is complete\n",
        "!ls -la /content/\n",
        "\n",
        "# ğŸ“ What you should see now:\n",
        "# - Your extracted files and folders\n",
        "# - NO dataset.zip (if you deleted it in step 7)\n",
        "# - drive/ folder (your Google Drive mount)\n",
        "# - sample_data/ folder (default Colab folder)\n",
        "\n",
        "# Check the total size of your extracted data\n",
        "!du -sh /content/\n",
        "\n",
        "# ğŸ“ du command explanation:\n",
        "# du = disk usage (shows how much space files use)\n",
        "# -s = summary (show total size, not individual files)\n",
        "# -h = human-readable (shows sizes like 1.2GB instead of 1200000000)"
      ],
      "metadata": {
        "id": "1OnO-cocdO6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning using trained model(Lane detection)"
      ],
      "metadata": {
        "id": "4_BQWaT_fEBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install roboflow ultralytics opencv-python\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from google.colab import files  # íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•œ import ì¶”ê°€\n",
        "# Roboflowì—ì„œ ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©\n",
        "#ì˜ìƒì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ **ì¶”ë¡ (inference)**ë§Œ ìˆ˜í–‰\n",
        "import cv2\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "\n",
        "def load_lane_detection_model():\n",
        "    \"\"\"\n",
        "    ì°¨ì„  ê°ì§€ ëª¨ë¸ ë¡œë“œ\n",
        "    \"\"\"\n",
        "    rf = Roboflow(api_key=\"JwvZQEBhBR5uPrwepqQW\")\n",
        "    #project = rf.workspace().project(\"0722_labeling-usrpl/1\")\n",
        "    project = rf.workspace().project(\"0722_labeling-usrpl\")\n",
        "    model = project.version(1).model\n",
        "    print(\"ğŸ›£ï¸ ì°¨ì„  ê°ì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "    return model\n",
        "\n",
        "def upload_video_files():\n",
        "    \"\"\"\n",
        "    PCì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ¬ Please select your video files to upload:\")\n",
        "    uploaded = files.upload()  # Returns a dictionary: {filename: file_content}\n",
        "\n",
        "    # Check if any files were actually uploaded\n",
        "    if not uploaded:\n",
        "        print(\"âŒ No files uploaded!\")\n",
        "        return None  # Return None if no files uploaded\n",
        "\n",
        "    print(f\"\\nâœ… Uploaded {len(uploaded)} file(s)\")\n",
        "\n",
        "    # ì—…ë¡œë“œëœ íŒŒì¼ ì¤‘ ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ íŒŒì¼ ë°˜í™˜\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv')):\n",
        "            print(f\"ğŸ“ Using video file: {filename}\")\n",
        "            return filename\n",
        "\n",
        "    print(\"âŒ No valid video files found in upload!\")\n",
        "    return None\n",
        "\n",
        "def detect_lanes_in_video(video_path, model, output_path=\"lane_detection_result.mp4\"):\n",
        "    \"\"\"\n",
        "    ì˜ìƒì—ì„œ ì°¨ì„  ê°ì§€\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # ì˜ìƒ ì •ë³´\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"ğŸ¬ ì˜ìƒ ì •ë³´: {width}x{height}, {fps}fps, {total_frames}í”„ë ˆì„\")\n",
        "\n",
        "    # ì¶œë ¥ ì„¤ì •\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    lane_detections = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # ë§¤ 3í”„ë ˆì„ë§ˆë‹¤ ì°¨ì„  ê°ì§€ (ë” ìì£¼)\n",
        "        if frame_count % 2 == 0:\n",
        "            try:\n",
        "                temp_img_path = \"temp_frame.jpg\"\n",
        "                cv2.imwrite(temp_img_path, frame)\n",
        "\n",
        "                # ì°¨ì„  ê°ì§€ (ë‚®ì€ ì‹ ë¢°ë„)\n",
        "                prediction = model.predict(temp_img_path, confidence=30, overlap=50)\n",
        "                predictions = prediction.json()['predictions']\n",
        "\n",
        "                frame_lanes = len(predictions)\n",
        "                lane_detections += frame_lanes\n",
        "\n",
        "                if frame_lanes > 0:\n",
        "                    print(f\"ğŸ›£ï¸ í”„ë ˆì„ {frame_count}: {frame_lanes}ê°œ ì°¨ì„  ê°ì§€\")\n",
        "\n",
        "                # ì°¨ì„  ê·¸ë¦¬ê¸°\n",
        "                for lane in predictions:\n",
        "                    x1 = int(lane['x'] - lane['width']/2)\n",
        "                    y1 = int(lane['y'] - lane['height']/2)\n",
        "                    x2 = int(lane['x'] + lane['width']/2)\n",
        "                    y2 = int(lane['y'] + lane['height']/2)\n",
        "\n",
        "                    # ì°¨ì„ ì€ ë³´ë¼ìƒ‰ìœ¼ë¡œ í‘œì‹œ\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
        "\n",
        "                    # ë¼ë²¨\n",
        "                    label = f\"Lane: {lane['confidence']:.2f}\"\n",
        "                    cv2.rectangle(frame, (x1, y1-30), (x1+150, y1), (255, 0, 255), -1)\n",
        "                    cv2.putText(frame, label, (x1+5, y1-8),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "                if os.path.exists(temp_img_path):\n",
        "                    os.remove(temp_img_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ í”„ë ˆì„ {frame_count} ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "        # ì§„í–‰ìƒí™©\n",
        "        if frame_count % 150 == 0:\n",
        "            progress = (frame_count / total_frames) * 100\n",
        "            print(f\"ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% (ì´ ì°¨ì„  ê°ì§€: {lane_detections}ê°œ)\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"âœ… ì™„ë£Œ! ì´ {lane_detections}ê°œ ì°¨ì„  ê°ì§€\")\n",
        "    print(f\"ğŸ¥ ê²°ê³¼: {output_path}\")\n",
        "\n",
        "def main_lane_detection():\n",
        "    \"\"\"\n",
        "    ì°¨ì„  ê°ì§€ ë©”ì¸ í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    print(\"ğŸš— ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘...\")\n",
        "    video_path = upload_video_files()\n",
        "\n",
        "    if video_path:\n",
        "        print(f\"ğŸ“ ì˜ìƒ íŒŒì¼: {video_path}\")\n",
        "\n",
        "        print(\"ğŸ›£ï¸ ì°¨ì„  ê°ì§€ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "        try:\n",
        "            model = load_lane_detection_model()\n",
        "\n",
        "            print(\"ğŸ” ì°¨ì„  ê°ì§€ ì‹œì‘...\")\n",
        "            detect_lanes_in_video(video_path, model, \"lane_detection_result.mp4\")\n",
        "\n",
        "            print(\"ğŸ‰ ì°¨ì„  ê°ì§€ ì™„ë£Œ!\")\n",
        "            print(\"ğŸ“º ê²°ê³¼ ì˜ìƒ: lane_detection_result.mp4\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "            print(\"ğŸ”‘ API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”!\")\n",
        "    else:\n",
        "        print(\"âŒ ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "def test_with_webcam_url():\n",
        "    \"\"\"\n",
        "    Roboflow Visualize í˜ì´ì§€ì˜ 'Paste YouTube or Image URL' ê¸°ëŠ¥ ì‚¬ìš©\n",
        "    \"\"\"\n",
        "    print(\"ğŸŒ ì›¹ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸:\")\n",
        "    print(\"1. Roboflow Visualize í˜ì´ì§€ì—ì„œ\")\n",
        "    print(\"2. 'Paste YouTube or Image URL' ì…ë ¥ì°½ì—\")\n",
        "    print(\"3. YouTube URL ë¶™ì—¬ë„£ê¸°\")\n",
        "    print(\"4. ì°¨ì„ ì´ ì˜ ê°ì§€ë˜ëŠ”ì§€ í™•ì¸\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸ›£ï¸ ì°¨ì„  ê°ì§€ ëª¨ë“œë¡œ ë³€ê²½!\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ğŸ’¡ ì´ ëª¨ë¸ì€ ì°¨ì„ (lane)ì„ ê°ì§€í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\")\n",
        "    print(\"ğŸ“¹ ë“œë¼ì´ë¹™ ì˜ìƒì´ë‚˜ ë„ë¡œ ì˜ìƒì—ì„œ ê°€ì¥ ì˜ ì‘ë™í•©ë‹ˆë‹¤.\")\n",
        "    print()\n",
        "    print(\"ğŸš€ ì‹¤í–‰ ë°©ë²•:\")\n",
        "    print(\"1. API í‚¤ ì…ë ¥\")\n",
        "    print(\"2. main_lane_detection() ì‹¤í–‰\")\n",
        "    print(\"3. PCì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ì„ íƒ\")\n",
        "    print(\"4. ë˜ëŠ” Roboflow ì›¹ì—ì„œ test_with_webcam_url() ë°©ë²• ì‹œë„\")\n",
        "    print(\"=\" * 50)\n",
        "    main_lane_detection()"
      ],
      "metadata": {
        "id": "HnZ72k2bz3tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning using trained model(General object detection)"
      ],
      "metadata": {
        "id": "-NJs885dfRvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (REVISED: Added YOLOv8 for comprehensive object detection)\n",
        "!pip install roboflow ultralytics opencv-python\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "import numpy as np\n",
        "from ultralytics import YOLO  # REVISED: Using YOLOv8 for multi-object detection\n",
        "from google.colab import files  # íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•œ import ì¶”ê°€\n",
        "# REVISED: Now supports detection of traffic signs, traffic lights, crosswalks, lanes, humans and vehicles\n",
        "# ì˜ìƒì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ **ë‹¤ì¤‘ ê°ì²´ ì¶”ë¡ (multi-object inference)** ìˆ˜í–‰\n",
        "import cv2\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# REVISED: Added comprehensive object detection configuration\n",
        "DETECTION_CONFIG = {\n",
        "    # Object colors for visualization (BGR format)\n",
        "    'colors': {\n",
        "        'person': (0, 255, 0),          # Green for humans\n",
        "        'car': (255, 0, 0),             # Blue for cars\n",
        "        'truck': (255, 0, 150),         # Purple for trucks\n",
        "        'bus': (255, 100, 0),           # Orange for buses\n",
        "        'motorcycle': (0, 255, 255),    # Yellow for motorcycles\n",
        "        'bicycle': (255, 255, 0),       # Cyan for bicycles\n",
        "        'traffic light': (0, 0, 255),   # Red for traffic lights\n",
        "        'stop sign': (0, 150, 255),     # Orange-red for stop signs\n",
        "        'lane': (255, 0, 255),          # Magenta for lanes\n",
        "        'crosswalk': (255, 255, 255),   # White for crosswalks\n",
        "        'default': (128, 128, 128)      # Gray for other objects\n",
        "    },\n",
        "\n",
        "    # Traffic-related object classes from YOLO\n",
        "    'traffic_classes': [\n",
        "        'person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck',\n",
        "        'traffic light', 'stop sign'\n",
        "    ],\n",
        "\n",
        "    # Confidence thresholds for different object types\n",
        "    'confidence_thresholds': {\n",
        "        'yolo': 0.5,      # 50% confidence for YOLO detections\n",
        "        'lane': 0.3       # 30% confidence for lane detections\n",
        "    }\n",
        "}\n",
        "\n",
        "def load_detection_models():\n",
        "    \"\"\"\n",
        "    REVISED: Load both YOLOv8 and lane detection models\n",
        "    ë‹¤ì¤‘ ëª¨ë¸ ë¡œë“œ - YOLO(ì¼ë°˜ ê°ì²´) + Roboflow(ì°¨ì„ )\n",
        "    \"\"\"\n",
        "    print(\"ğŸ¤– Loading comprehensive detection models...\")\n",
        "\n",
        "    # 1. Load YOLOv8 model for general traffic objects\n",
        "    print(\"  ğŸ“¦ Loading YOLOv8 model for traffic objects...\")\n",
        "    yolo_model = YOLO('yolov8n.pt')  # Nano version for speed, use 'yolov8s.pt' for better accuracy\n",
        "    print(\"  âœ… YOLOv8 model loaded successfully!\")\n",
        "\n",
        "    # 2. Load Roboflow model for lane detection\n",
        "    print(\"  ğŸ›£ï¸  Loading specialized lane detection model...\")\n",
        "    try:\n",
        "        rf = Roboflow(api_key=\"JwvZQEBhBR5uPrwepqQW\")\n",
        "        project = rf.workspace().project(\"0722_labeling-usrpl\")\n",
        "        lane_model = project.version(1).model\n",
        "        print(\"  âœ… Lane detection model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸  Lane model loading failed: {e}\")\n",
        "        print(\"  ğŸ“ Continuing with YOLOv8 only...\")\n",
        "        lane_model = None\n",
        "\n",
        "    print(\"ğŸ¯ All available models loaded!\")\n",
        "    return yolo_model, lane_model\n",
        "\n",
        "def upload_video_files():\n",
        "    \"\"\"\n",
        "    PCì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ (Unchanged)\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ¬ Please select your video files to upload:\")\n",
        "    uploaded = files.upload()  # Returns a dictionary: {filename: file_content}\n",
        "\n",
        "    # Check if any files were actually uploaded\n",
        "    if not uploaded:\n",
        "        print(\"âŒ No files uploaded!\")\n",
        "        return None  # Return None if no files uploaded\n",
        "\n",
        "    print(f\"\\nâœ… Uploaded {len(uploaded)} file(s)\")\n",
        "\n",
        "    # ì—…ë¡œë“œëœ íŒŒì¼ ì¤‘ ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ íŒŒì¼ ë°˜í™˜\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv')):\n",
        "            print(f\"ğŸ“ Using video file: {filename}\")\n",
        "            return filename\n",
        "\n",
        "    print(\"âŒ No valid video files found in upload!\")\n",
        "    return None\n",
        "\n",
        "def detect_yolo_objects(frame, yolo_model):\n",
        "    \"\"\"\n",
        "    REVISED: YOLOv8ì„ ì‚¬ìš©í•œ êµí†µ ê°ì²´ ê°ì§€\n",
        "    Detect traffic-related objects using YOLOv8\n",
        "    \"\"\"\n",
        "    detections = []\n",
        "\n",
        "    try:\n",
        "        # Run YOLOv8 inference\n",
        "        results = yolo_model(frame, conf=DETECTION_CONFIG['confidence_thresholds']['yolo'])\n",
        "\n",
        "        # Parse results\n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            if boxes is not None:\n",
        "                for box in boxes:\n",
        "                    # Get class name\n",
        "                    class_id = int(box.cls[0])\n",
        "                    class_name = yolo_model.names[class_id]\n",
        "\n",
        "                    # Only keep traffic-related objects\n",
        "                    if class_name in DETECTION_CONFIG['traffic_classes']:\n",
        "                        confidence = float(box.conf[0])\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "\n",
        "                        detections.append({\n",
        "                            'class': class_name,\n",
        "                            'confidence': confidence,\n",
        "                            'bbox': (int(x1), int(y1), int(x2), int(y2)),\n",
        "                            'type': 'yolo'\n",
        "                        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ YOLOv8 detection error: {e}\")\n",
        "\n",
        "    return detections\n",
        "\n",
        "def detect_lane_objects(frame, lane_model):\n",
        "    \"\"\"\n",
        "    REVISED: Roboflowë¥¼ ì‚¬ìš©í•œ ì°¨ì„  ê°ì§€\n",
        "    Detect lanes using specialized Roboflow model\n",
        "    \"\"\"\n",
        "    detections = []\n",
        "\n",
        "    if lane_model is None:\n",
        "        return detections\n",
        "\n",
        "    try:\n",
        "        # Save frame temporarily for Roboflow processing\n",
        "        temp_img_path = \"temp_frame_lane.jpg\"\n",
        "        cv2.imwrite(temp_img_path, frame)\n",
        "\n",
        "        # Run lane detection\n",
        "        prediction = lane_model.predict(\n",
        "            temp_img_path,\n",
        "            confidence=int(DETECTION_CONFIG['confidence_thresholds']['lane'] * 100),\n",
        "            overlap=50\n",
        "        )\n",
        "        predictions = prediction.json()['predictions']\n",
        "\n",
        "        # Parse lane detections\n",
        "        for lane in predictions:\n",
        "            x1 = int(lane['x'] - lane['width']/2)\n",
        "            y1 = int(lane['y'] - lane['height']/2)\n",
        "            x2 = int(lane['x'] + lane['width']/2)\n",
        "            y2 = int(lane['y'] + lane['height']/2)\n",
        "\n",
        "            detections.append({\n",
        "                'class': 'lane',\n",
        "                'confidence': lane['confidence'] / 100.0,  # Convert to 0-1 scale\n",
        "                'bbox': (x1, y1, x2, y2),\n",
        "                'type': 'lane'\n",
        "            })\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_img_path):\n",
        "            os.remove(temp_img_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Lane detection error: {e}\")\n",
        "\n",
        "    return detections\n",
        "\n",
        "def draw_detections(frame, detections):\n",
        "    \"\"\"\n",
        "    REVISED: ëª¨ë“  ê°ì§€ëœ ê°ì²´ë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°\n",
        "    Draw all detected objects on frame with different colors\n",
        "    \"\"\"\n",
        "    detection_counts = {}\n",
        "\n",
        "    for detection in detections:\n",
        "        class_name = detection['class']\n",
        "        confidence = detection['confidence']\n",
        "        x1, y1, x2, y2 = detection['bbox']\n",
        "\n",
        "        # Count detections by class\n",
        "        detection_counts[class_name] = detection_counts.get(class_name, 0) + 1\n",
        "\n",
        "        # Get color for this object class\n",
        "        color = DETECTION_CONFIG['colors'].get(class_name, DETECTION_CONFIG['colors']['default'])\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "        # Prepare label\n",
        "        label = f\"{class_name}: {confidence:.2f}\"\n",
        "\n",
        "        # Calculate label background size\n",
        "        (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "\n",
        "        # Draw label background\n",
        "        cv2.rectangle(frame, (x1, y1-25), (x1 + label_width + 10, y1), color, -1)\n",
        "\n",
        "        # Draw label text\n",
        "        cv2.putText(frame, label, (x1 + 5, y1 - 5),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "    return detection_counts\n",
        "\n",
        "def detect_comprehensive_objects_in_video(video_path, yolo_model, lane_model, output_path=\"comprehensive_detection_result.mp4\"):\n",
        "    \"\"\"\n",
        "    REVISED: ì˜ìƒì—ì„œ ì¢…í•©ì ì¸ ë„ë¡œ ê°ì²´ ê°ì§€\n",
        "    Comprehensive road object detection in video\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # ì˜ìƒ ì •ë³´ í™•ì¸\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"ğŸ¬ Video info: {width}x{height}, {fps}fps, {total_frames} frames\")\n",
        "\n",
        "    # ì¶œë ¥ ì„¤ì •\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # REVISED: Tracking variables for comprehensive detection\n",
        "    frame_count = 0\n",
        "    total_detections = {\n",
        "        'person': 0, 'car': 0, 'truck': 0, 'bus': 0, 'motorcycle': 0,\n",
        "        'bicycle': 0, 'traffic light': 0, 'stop sign': 0, 'lane': 0\n",
        "    }\n",
        "\n",
        "    print(\"ğŸ” Starting comprehensive object detection...\")\n",
        "    print(\"ğŸ“Š Detection progress:\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        frame_detections = []\n",
        "\n",
        "        # REVISED: Process every 3rd frame for efficiency (can be adjusted)\n",
        "        if frame_count % 3 == 0:\n",
        "            print(f\"    ğŸ” Processing frame {frame_count}/{total_frames}...\")\n",
        "\n",
        "            # 1. YOLO Object Detection\n",
        "            print(\"      ğŸš— Running traffic object detection...\")\n",
        "            yolo_detections = detect_yolo_objects(frame, yolo_model)\n",
        "            frame_detections.extend(yolo_detections)\n",
        "\n",
        "            # 2. Lane Detection\n",
        "            print(\"      ğŸ›£ï¸  Running lane detection...\")\n",
        "            lane_detections = detect_lane_objects(frame, lane_model)\n",
        "            frame_detections.extend(lane_detections)\n",
        "\n",
        "            # 3. Draw all detections\n",
        "            frame_counts = draw_detections(frame, frame_detections)\n",
        "\n",
        "            # 4. Update total counts\n",
        "            for obj_class, count in frame_counts.items():\n",
        "                if obj_class in total_detections:\n",
        "                    total_detections[obj_class] += count\n",
        "\n",
        "            # 5. Print frame summary\n",
        "            if frame_detections:\n",
        "                detected_objects = list(frame_counts.keys())\n",
        "                print(f\"      âœ… Frame {frame_count}: Found {len(frame_detections)} objects - {', '.join(detected_objects)}\")\n",
        "\n",
        "        # Write frame to output video\n",
        "        out.write(frame)\n",
        "\n",
        "        # REVISED: Progress reporting every 5%\n",
        "        if frame_count % max(1, total_frames // 20) == 0:\n",
        "            progress = (frame_count / total_frames) * 100\n",
        "            print(f\"\\nğŸ“ˆ Progress: {progress:.1f}% complete\")\n",
        "            print(\"ğŸ¯ Detection summary so far:\")\n",
        "            for obj_type, count in total_detections.items():\n",
        "                if count > 0:\n",
        "                    print(f\"    {obj_type}: {count}\")\n",
        "            print()\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # REVISED: Final comprehensive summary\n",
        "    print(\"ğŸ‰ Comprehensive detection completed!\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ğŸ“Š FINAL DETECTION SUMMARY:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for obj_type, count in total_detections.items():\n",
        "        if count > 0:\n",
        "            print(f\"ğŸ”¸ {obj_type.title()}: {count} detections\")\n",
        "\n",
        "    total_objects = sum(total_detections.values())\n",
        "    print(f\"\\nğŸ¯ Total objects detected: {total_objects}\")\n",
        "    print(f\"ğŸ¥ Output video: {output_path}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "def main_comprehensive_detection():\n",
        "    \"\"\"\n",
        "    REVISED: ì¢…í•©ì ì¸ ë„ë¡œ ê°ì²´ ê°ì§€ ë©”ì¸ í•¨ìˆ˜\n",
        "    Main function for comprehensive road object detection\n",
        "    \"\"\"\n",
        "    print(\"ğŸš— Starting comprehensive road object detection...\")\n",
        "    print(\"ğŸ¯ Will detect: Traffic signs, lights, crosswalks, lanes, humans, and vehicles\")\n",
        "\n",
        "    # Step 1: Upload video file\n",
        "    video_path = upload_video_files()\n",
        "\n",
        "    if video_path:\n",
        "        print(f\"ğŸ“ Video file: {video_path}\")\n",
        "\n",
        "        # Step 2: Load detection models\n",
        "        print(\"\\nğŸ¤– Loading detection models...\")\n",
        "        try:\n",
        "            yolo_model, lane_model = load_detection_models()\n",
        "\n",
        "            # Step 3: Run comprehensive detection\n",
        "            print(\"\\nğŸ” Starting comprehensive object detection...\")\n",
        "            detect_comprehensive_objects_in_video(\n",
        "                video_path, yolo_model, lane_model,\n",
        "                \"comprehensive_detection_result.mp4\"\n",
        "            )\n",
        "\n",
        "            print(\"\\nğŸ‰ Comprehensive detection completed!\")\n",
        "            print(\"ğŸ“º Result video: comprehensive_detection_result.mp4\")\n",
        "            print(\"\\nğŸ¨ Color coding:\")\n",
        "            for obj_type, color in DETECTION_CONFIG['colors'].items():\n",
        "                if obj_type != 'default':\n",
        "                    print(f\"  {obj_type}: {color}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Model loading failed: {e}\")\n",
        "            print(\"ğŸ”‘ Please check your API key and internet connection!\")\n",
        "    else:\n",
        "        print(\"âŒ No video file uploaded.\")\n",
        "\n",
        "def test_with_webcam_url():\n",
        "    \"\"\"\n",
        "    Roboflow Visualize í˜ì´ì§€ì˜ 'Paste YouTube or Image URL' ê¸°ëŠ¥ ì‚¬ìš© (Unchanged)\n",
        "    \"\"\"\n",
        "    print(\"ğŸŒ ì›¹ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸:\")\n",
        "    print(\"1. Roboflow Visualize í˜ì´ì§€ì—ì„œ\")\n",
        "    print(\"2. 'Paste YouTube or Image URL' ì…ë ¥ì°½ì—\")\n",
        "    print(\"3. YouTube URL ë¶™ì—¬ë„£ê¸°\")\n",
        "    print(\"4. ì°¨ì„ ì´ ì˜ ê°ì§€ë˜ëŠ”ì§€ í™•ì¸\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸš¦ COMPREHENSIVE ROAD OBJECT DETECTION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ¯ DETECTABLE OBJECTS:\")\n",
        "    print(\"  ğŸ‘¤ Humans (persons)\")\n",
        "    print(\"  ğŸš— Vehicles (cars, trucks, buses, motorcycles, bicycles)\")\n",
        "    print(\"  ğŸš¦ Traffic lights\")\n",
        "    print(\"  ğŸ›‘ Traffic signs (stop signs)\")\n",
        "    print(\"  ğŸ›£ï¸  Road lanes\")\n",
        "    print(\"  ğŸš¶ Crosswalks (when visible)\")\n",
        "    print()\n",
        "    print(\"ğŸ¨ VISUAL CODING:\")\n",
        "    print(\"  Different colors for different object types\")\n",
        "    print(\"  Confidence scores displayed for each detection\")\n",
        "    print()\n",
        "    print(\"ğŸš€ EXECUTION STEPS:\")\n",
        "    print(\"1. Upload your driving video file\")\n",
        "    print(\"2. Models will load automatically\")\n",
        "    print(\"3. Comprehensive detection will run\")\n",
        "    print(\"4. Results saved as comprehensive_detection_result.mp4\")\n",
        "    print(\"=\" * 60)\n",
        "    main_comprehensive_detection()"
      ],
      "metadata": {
        "id": "MJg4OpJVfITL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the contents of dataset.yaml"
      ],
      "metadata": {
        "id": "_NeRfugzfonO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. dataset.yaml íŒŒì¼ ë‚´ìš© í™•ì¸\n",
        "print(\"ğŸ“‹ dataset.yaml íŒŒì¼ ë‚´ìš©:\")\n",
        "with open('/content/dataset/dataset.yaml', 'r') as f:\n",
        "    yaml_content = f.read()\n",
        "    print(yaml_content)"
      ],
      "metadata": {
        "id": "pqn9bfQjfm06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trasfer Learning(perception) using YOLO11 and pre-traind YOLO"
      ],
      "metadata": {
        "id": "NgsEQKJGfoLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for YOLO and video processing\n",
        "!pip install ultralytics yt-dlp\n",
        "\n",
        "# Import all necessary libraries\n",
        "from ultralytics import YOLO           # YOLO model for object detection\n",
        "import glob                            # File path pattern matching\n",
        "import cv2                             # OpenCV for video processing\n",
        "import numpy as np                     # Numerical operations\n",
        "from IPython.display import Video      # Display videos in Jupyter notebook\n",
        "import shutil                          # File operations (copy, move files)\n",
        "from google.colab import files         # File upload functionality for Google Colab\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: CREATE DATASET CONFIGURATION FILE\n",
        "# ============================================================================\n",
        "# This YAML configuration file defines the dataset structure for the custom model\n",
        "# It specifies where training/validation images are located and what classes exist\n",
        "yaml_fix = '''path: /content/dataset\n",
        "train: train/images\n",
        "val: valid/images\n",
        "names:\n",
        "  0: lane                # Class 0: Lane detection\n",
        "  1: traffic_sign        # Class 1: Traffic sign detection\n",
        "nc: 2'''                 # Number of classes (nc = number of classes)\n",
        "\n",
        "# Write the configuration to a file that YOLO can read\n",
        "with open('/content/dataset/dataset_fixed.yaml', 'w') as f:\n",
        "    f.write(yaml_fix)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: LOAD YOLO MODELS\n",
        "# ============================================================================\n",
        "print(\"ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "print(\"   - ê¸°ë³¸ YOLO ëª¨ë¸: ì¼ë°˜ì ì¸ 80ê°œ í´ë˜ìŠ¤ ê°ì²´ íƒì§€ (ì‚¬ëŒ, ì°¨ëŸ‰, ì‹ í˜¸ë“± ë“±)\")\n",
        "print(\"   - ì»¤ìŠ¤í…€ YOLO ëª¨ë¸: íŠ¹ë³„íˆ í•™ìŠµëœ 2ê°œ í´ë˜ìŠ¤ íƒì§€ (ì°¨ì„ , êµí†µí‘œì§€íŒ)\")\n",
        "\n",
        "# Load the base YOLO11n model (pre-trained on COCO dataset with 80 classes)\n",
        "base_model = YOLO('yolo11n.pt')        # Detects: person, car, truck, traffic light, etc.\n",
        "\n",
        "# Load your custom trained model (specialized for 2 classes)\n",
        "custom_model = YOLO('/content/dataset/best.pt')  # Detects: lane, traffic_sign\n",
        "\n",
        "print(f\"ğŸ“‹ ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {len(base_model.names)} (COCO ë°ì´í„°ì…‹ ê¸°ë°˜)\")\n",
        "print(f\"ğŸ“‹ ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {len(custom_model.names)} (ì°¨ì„ , êµí†µí‘œì§€íŒ ì „ìš©)\")\n",
        "\n",
        "# Display what classes each model can detect\n",
        "print(f\"   ğŸ” ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ ì˜ˆì‹œ: {list(base_model.names.values())[:10]}...\")  # Show first 10 classes\n",
        "print(f\"   ğŸ” ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤: {list(custom_model.names.values())}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: UPLOAD VIDEO FILES FROM PC\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ“¥ ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ\")\n",
        "print(\"   PCì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì„ íƒí•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”\")\n",
        "print(\"   ì§€ì› í˜•ì‹: MP4, AVI, MOV, MKV ë“± ëŒ€ë¶€ë¶„ì˜ ë¹„ë””ì˜¤ í˜•ì‹\")\n",
        "\n",
        "# Open file upload dialog for users to select video files from their PC\n",
        "print(\"\\nğŸ¬ ì—…ë¡œë“œí•  ë¹„ë””ì˜¤ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”:\")\n",
        "uploaded = files.upload()  # Returns a dictionary: {filename: file_content}\n",
        "\n",
        "# Check if any files were actually uploaded\n",
        "if not uploaded:\n",
        "    print(\"âŒ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "    raise Exception(\"No files uploaded - ì—…ë¡œë“œë¥¼ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”\")\n",
        "\n",
        "print(f\"\\nâœ… {len(uploaded)} ê°œì˜ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤\")\n",
        "\n",
        "# Get the first uploaded video file path\n",
        "# The uploaded files are automatically saved to the current directory\n",
        "uploaded_filenames = list(uploaded.keys())\n",
        "video_path = uploaded_filenames[0]  # Use the first uploaded file\n",
        "print(f\"ğŸ“¹ ì²˜ë¦¬í•  ë¹„ë””ì˜¤: {video_path}\")\n",
        "print(f\"ğŸ“ íŒŒì¼ í¬ê¸°: {len(uploaded[video_path]) / (1024*1024):.1f} MB\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: COMBINED INFERENCE FUNCTION\n",
        "# ============================================================================\n",
        "def combined_inference(video_path, output_path='/content/combined_result.mp4'):\n",
        "    \"\"\"\n",
        "    Combine results from both base YOLO and custom YOLO models\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to input video file\n",
        "        output_path: Path where the result video will be saved\n",
        "\n",
        "    Process:\n",
        "        1. Read video frame by frame\n",
        "        2. Run both models on each frame\n",
        "        3. Draw detection boxes with different colors\n",
        "        4. Save processed video\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nğŸ¬ ê²°í•©ëœ ì¶”ë¡  ì‹œì‘: {video_path}\")\n",
        "    print(\"   ë‘ ê°œì˜ YOLO ëª¨ë¸ì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ëª¨ë“  ê°ì²´ë¥¼ íƒì§€í•©ë‹ˆë‹¤\")\n",
        "\n",
        "    # ========== Video Properties Setup ==========\n",
        "    cap = cv2.VideoCapture(video_path)           # Open video file for reading\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))         # Get original video frame rate\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   # Get video width in pixels\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Get video height in pixels\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Total number of frames\n",
        "\n",
        "    print(f\"   ğŸ“¹ ë¹„ë””ì˜¤ ì •ë³´: {width}x{height} í•´ìƒë„, {fps} FPS, {total_frames} ì´ í”„ë ˆì„\")\n",
        "\n",
        "    # Setup output video writer with same properties as input video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')     # Video codec for MP4 format\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # ========== Frame Processing Loop ==========\n",
        "    frame_count = 0\n",
        "    print(f\"ğŸ”„ ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ì´ {total_frames} í”„ë ˆì„)\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()  # Read next frame from video\n",
        "        if not ret:              # If no more frames, break the loop\n",
        "            break\n",
        "\n",
        "        # ========== Run Both YOLO Models ==========\n",
        "        # Base YOLO inference: detects general objects (cars, people, traffic lights, etc.)\n",
        "        base_results = base_model(frame, verbose=False)    # verbose=False: suppress output messages\n",
        "\n",
        "        # Custom YOLO inference: detects specialized objects (lanes, traffic signs)\n",
        "        custom_results = custom_model(frame, verbose=False)\n",
        "\n",
        "        # ========== Prepare Frame for Annotation ==========\n",
        "        annotated_frame = frame.copy()  # Create a copy to draw on (preserve original)\n",
        "\n",
        "        # ========== Draw Base YOLO Results (BLUE boxes) ==========\n",
        "        if base_results[0].boxes is not None:  # Check if any objects were detected\n",
        "            for box in base_results[0].boxes:\n",
        "                # Extract bounding box coordinates (x1, y1) = top-left, (x2, y2) = bottom-right\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])        # Confidence score (0.0 to 1.0)\n",
        "                cls = int(box.cls[0])            # Class index number\n",
        "\n",
        "                # Only draw boxes with confidence > 30% to reduce false positives\n",
        "                if conf > 0.3:\n",
        "                    # Create label text with class name and confidence percentage\n",
        "                    label = f\"{base_model.names[cls]} {conf:.2f}\"\n",
        "\n",
        "                    # Draw blue rectangle around detected object\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Blue color (B, G, R)\n",
        "\n",
        "                    # Draw label text above the rectangle\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # ========== Draw Custom YOLO Results (RED boxes) ==========\n",
        "        if custom_results[0].boxes is not None:  # Check if any objects were detected\n",
        "            for box in custom_results[0].boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                # Only draw boxes with confidence > 30%\n",
        "                if conf > 0.3:\n",
        "                    # Create label text with class name and confidence\n",
        "                    label = f\"{custom_model.names[cls]} {conf:.2f}\"\n",
        "\n",
        "                    # Draw red rectangle around detected object\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Red color (B, G, R)\n",
        "\n",
        "                    # Draw label text above the rectangle\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # ========== Save Processed Frame ==========\n",
        "        out.write(annotated_frame)  # Write the annotated frame to output video\n",
        "        frame_count += 1\n",
        "\n",
        "        # Progress reporting every 30 frames to avoid overwhelming the console\n",
        "        if frame_count % 30 == 0:\n",
        "            progress_percent = (frame_count / total_frames) * 100\n",
        "            print(f\"   ğŸ”„ ì§„í–‰ìƒí™©: {frame_count}/{total_frames} ({progress_percent:.1f}%)\")\n",
        "\n",
        "    # ========== Cleanup and Finalize ==========\n",
        "    cap.release()   # Close input video file\n",
        "    out.release()   # Close output video file and finalize writing\n",
        "\n",
        "    print(f\"âœ… ê²°í•© ê²°ê³¼ ì˜ìƒ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
        "    print(f\"ğŸ“Š ì²˜ë¦¬ëœ ì´ í”„ë ˆì„: {frame_count}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: INDIVIDUAL MODEL INFERENCE RESULTS\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ” ê°œë³„ ëª¨ë¸ ì¶”ë¡  ê²°ê³¼ ìƒì„±:\")\n",
        "print(\"   ê° ëª¨ë¸ì˜ ê°œë³„ ì„±ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ë”°ë¡œë”°ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤\")\n",
        "\n",
        "# ========== Custom Model Only Inference ==========\n",
        "print(\"\\n1ï¸âƒ£ ì»¤ìŠ¤í…€ ëª¨ë¸ ë‹¨ë… ì‹¤í–‰ (ì°¨ì„ , êµí†µí‘œì§€íŒë§Œ íƒì§€):\")\n",
        "print(\"   ì´ ëª¨ë¸ì€ íŠ¹ë³„íˆ ì°¨ì„ ê³¼ êµí†µí‘œì§€íŒ íƒì§€ë¥¼ ìœ„í•´ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤\")\n",
        "custom_results = custom_model(\n",
        "    video_path,                    # Input video path\n",
        "    save=True,                     # Save results automatically\n",
        "    project='/content',            # Project directory for saving\n",
        "    name='custom_only',            # Folder name for this run\n",
        "    conf=0.3                       # Minimum confidence threshold\n",
        ")\n",
        "\n",
        "# ========== Base Model Only Inference ==========\n",
        "print(\"\\n2ï¸âƒ£ ê¸°ë³¸ ëª¨ë¸ ë‹¨ë… ì‹¤í–‰ (ì¼ë°˜ ê°ì²´ë“¤ íƒì§€):\")\n",
        "print(\"   COCO ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ë¡œ 80ê°œ í´ë˜ìŠ¤ì˜ ì¼ë°˜ì ì¸ ê°ì²´ë¥¼ íƒì§€í•©ë‹ˆë‹¤\")\n",
        "base_results = base_model(\n",
        "    video_path,                    # Input video path\n",
        "    save=True,                     # Save results automatically\n",
        "    project='/content',            # Project directory for saving\n",
        "    name='base_only',              # Folder name for this run\n",
        "    conf=0.3                       # Minimum confidence threshold\n",
        ")\n",
        "\n",
        "# ========== Combined Model Inference ==========\n",
        "print(\"\\n3ï¸âƒ£ ê²°í•© ëª¨ë¸ ì‹¤í–‰ (ëª¨ë“  ê°ì²´ ë™ì‹œ íƒì§€):\")\n",
        "print(\"   ë‘ ëª¨ë¸ì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ëª¨ë“  ì¢…ë¥˜ì˜ ê°ì²´ë¥¼ íƒì§€í•©ë‹ˆë‹¤\")\n",
        "print(\"   íŒŒë€ìƒ‰ ë°•ìŠ¤: ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼, ë¹¨ê°„ìƒ‰ ë°•ìŠ¤: ì»¤ìŠ¤í…€ ëª¨ë¸ ê²°ê³¼\")\n",
        "combined_inference(video_path, '/content/combined_result.mp4')\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: ORGANIZE RESULT FILES\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ“ ê²°ê³¼ íŒŒì¼ ì •ë¦¬ ì¤‘...\")\n",
        "print(\"   ìƒì„±ëœ ê²°ê³¼ íŒŒì¼ë“¤ì„ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ë³µì‚¬í•˜ì—¬ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤\")\n",
        "\n",
        "# Define mapping of standard names to actual generated file paths\n",
        "result_files = {\n",
        "    'custom_result.mp4': glob.glob('/content/custom_only/*.avi') + glob.glob('/content/custom_only/*.mp4'),\n",
        "    'base_result.mp4': glob.glob('/content/base_only/*.avi') + glob.glob('/content/base_only/*.mp4'),\n",
        "    'final_combined_result.mp4': ['/content/combined_result.mp4']\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“‹ ê²°ê³¼ íŒŒì¼ë“¤:\")\n",
        "for standard_name, file_list in result_files.items():\n",
        "    if file_list and file_list[0]:  # Check if files exist\n",
        "        try:\n",
        "            # Copy the first found file to a standard name\n",
        "            shutil.copy(file_list[0], f'/content/{standard_name}')\n",
        "            print(f\"âœ… {standard_name} ìƒì„± ì™„ë£Œ (ì›ë³¸: {file_list[0]})\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {standard_name} ìƒì„± ì‹¤íŒ¨: {e}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ {standard_name} í•´ë‹¹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: MODEL PERFORMANCE EVALUATION\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ“Š ì»¤ìŠ¤í…€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:\")\n",
        "print(\"   Validation ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤\")\n",
        "print(\"   mAP50ì€ IoU 0.5ì—ì„œì˜ í‰ê·  ì •ë°€ë„ë¡œ, ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•©ë‹ˆë‹¤\")\n",
        "\n",
        "try:\n",
        "    # Evaluate custom model performance on validation dataset\n",
        "    metrics = custom_model.val(data='/content/dataset/dataset_fixed.yaml')\n",
        "    print(f\"ğŸ¯ mAP50 (Mean Average Precision): {metrics.box.map50:.4f}\")\n",
        "    print(f\"   í•´ì„: {metrics.box.map50:.1%} ì •í™•ë„ë¡œ ê°ì²´ë¥¼ íƒì§€í•©ë‹ˆë‹¤\")\n",
        "\n",
        "    # Additional metrics if available\n",
        "    if hasattr(metrics.box, 'map'):\n",
        "        print(f\"ğŸ¯ mAP50-95 (ì „ì²´ IoU ë²”ìœ„): {metrics.box.map:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "    print(\"   ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ê±°ë‚˜ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: DISPLAY FINAL RESULTS\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ¬ ìµœì¢… ê²°í•© ê²°ê³¼ ì˜ìƒ:\")\n",
        "print(\"   ëª¨ë“  ê°ì²´ê°€ íƒì§€ëœ ìµœì¢… ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”\")\n",
        "\n",
        "try:\n",
        "    # Display the final combined result video\n",
        "    Video('/content/final_combined_result.mp4', width=800)\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ë¹„ë””ì˜¤ í‘œì‹œ ì˜¤ë¥˜: {e}\")\n",
        "    print(\"   íŒŒì¼ì´ ìƒì„±ë˜ì—ˆì§€ë§Œ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: RESULTS SUMMARY AND INTERPRETATION GUIDE\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ¯ ê²°ê³¼ ìš”ì•½ ë° í•´ì„ ê°€ì´ë“œ:\")\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“‹ íƒì§€ ê²°ê³¼ ìƒ‰ìƒ êµ¬ë¶„:\")\n",
        "print(\"ğŸ”µ íŒŒë€ìƒ‰ ë°•ìŠ¤: ê¸°ë³¸ YOLO ëª¨ë¸ ê²°ê³¼\")\n",
        "print(\"   â””â”€â”€ íƒì§€ ê°€ëŠ¥: ì‚¬ëŒ, ìë™ì°¨, íŠ¸ëŸ­, ë²„ìŠ¤, ì˜¤í† ë°”ì´, ìì „ê±°, ì‹ í˜¸ë“±, ì •ì§€í‘œì§€íŒ ë“±\")\n",
        "print(\"   â””â”€â”€ ì´ 80ê°œ í´ë˜ìŠ¤ì˜ ì¼ë°˜ì ì¸ ê°ì²´ë“¤\")\n",
        "print(\"ğŸ”´ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤: ì»¤ìŠ¤í…€ YOLO ëª¨ë¸ ê²°ê³¼\")\n",
        "print(\"   â””â”€â”€ íƒì§€ ê°€ëŠ¥: ì°¨ì„ (lane), êµí†µí‘œì§€íŒ(traffic_sign)\")\n",
        "print(\"   â””â”€â”€ ë„ë¡œ í™˜ê²½ íŠ¹í™” ê°ì²´ë“¤\")\n",
        "\n",
        "print(\"\\nğŸ“Š ì„±ëŠ¥ íŠ¹ì§•:\")\n",
        "print(\"â€¢ ê¸°ë³¸ ëª¨ë¸: ë„“ì€ ë²”ìœ„ì˜ ê°ì²´ íƒì§€, ì¼ë°˜ì ì¸ ìƒí™©ì— ê°•í•¨\")\n",
        "print(\"â€¢ ì»¤ìŠ¤í…€ ëª¨ë¸: íŠ¹ì • ë„ë©”ì¸ íŠ¹í™”, ì°¨ì„ /í‘œì§€íŒ íƒì§€ì— ë†’ì€ ì •í™•ë„\")\n",
        "print(\"â€¢ ê²°í•© ëª¨ë¸: ë‘ ëª¨ë¸ì˜ ì¥ì ì„ ëª¨ë‘ í™œìš©, í¬ê´„ì ì¸ ê°ì²´ íƒì§€\")\n",
        "\n",
        "print(\"\\nğŸ’¾ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ê²°ê³¼ íŒŒì¼ë“¤:\")\n",
        "print(\"ğŸ“ /content/ í´ë”ì— ì €ì¥ëœ íŒŒì¼ë“¤:\")\n",
        "print(\"â”œâ”€â”€ custom_result.mp4        : ì»¤ìŠ¤í…€ ëª¨ë¸ë§Œì˜ íƒì§€ ê²°ê³¼\")\n",
        "print(\"â”œâ”€â”€ base_result.mp4          : ê¸°ë³¸ ëª¨ë¸ë§Œì˜ íƒì§€ ê²°ê³¼\")\n",
        "print(\"â”œâ”€â”€ final_combined_result.mp4: ë‘ ëª¨ë¸ ê²°í•© ìµœì¢… ê²°ê³¼\")\n",
        "print(\"â””â”€â”€ ì›ë³¸ ì—…ë¡œë“œ íŒŒì¼ë„ í•¨ê»˜ ë³´ê´€ë¨\")\n",
        "\n",
        "print(\"\\nğŸš€ ì‚¬ìš© ê¶Œì¥ì‚¬í•­:\")\n",
        "print(\"â€¢ ì¼ë°˜ì ì¸ ê°ì²´ íƒì§€: base_result.mp4 ì‚¬ìš©\")\n",
        "print(\"â€¢ ë„ë¡œ/êµí†µ í™˜ê²½ ë¶„ì„: custom_result.mp4 ì‚¬ìš©\")\n",
        "print(\"â€¢ ì¢…í•©ì ì¸ ë¶„ì„: final_combined_result.mp4 ì‚¬ìš©\")\n",
        "\n",
        "print(\"\\nâœ¨ ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zO8V1XsmfpMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Compare inference speed between PyTorch and TensorRT models(with CPU)"
      ],
      "metadata": {
        "id": "l1QEksYFSalJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install ultralytics yt-dlp\n",
        "\n",
        "# Import all necessary libraries\n",
        "from ultralytics import YOLO           # YOLO model for object detection\n",
        "import glob                            # File path pattern matching\n",
        "import cv2                             # OpenCV for video processing\n",
        "import numpy as np                     # Numerical operations\n",
        "from IPython.display import Video      # Display videos in Jupyter\n",
        "import shutil                          # File operations\n",
        "import time                            # Time measurement for performance testing\n",
        "from google.colab import files         # File upload functionality for Google Colab\n",
        "\n",
        "# Create and save the dataset configuration file\n",
        "# This YAML file defines the dataset structure for custom model training/validation\n",
        "yaml_fix = '''path: /content/dataset\n",
        "train: train/images\n",
        "val: valid/images\n",
        "names:\n",
        "  0: lane\n",
        "  1: traffic_sign\n",
        "nc: 2'''\n",
        "\n",
        "# Write the configuration to a file\n",
        "with open('/content/dataset/dataset_fixed.yaml', 'w') as f:\n",
        "    f.write(yaml_fix)\n",
        "\n",
        "print(\"ğŸš€ TensorRT ìµœì í™” YOLO ì¶”ë¡  ì‹œì‘!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD BASE MODELS\n",
        "# ============================================================================\n",
        "print(\"ğŸ¤– ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "print(\"   - ê¸°ë³¸ YOLO11n ëª¨ë¸: ì¼ë°˜ì ì¸ ê°ì²´ íƒì§€ìš©\")\n",
        "print(\"   - ì»¤ìŠ¤í…€ ëª¨ë¸: ì°¨ì„  ë° êµí†µí‘œì§€íŒ ì „ìš© í•™ìŠµ ëª¨ë¸\")\n",
        "\n",
        "# Load the base YOLO11n model (pre-trained on COCO dataset)\n",
        "base_model = YOLO('yolo11n.pt')\n",
        "\n",
        "# Load your custom trained model for lane and traffic sign detection\n",
        "custom_model = YOLO('/content/dataset/best.pt')\n",
        "\n",
        "print(f\"ğŸ“‹ ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {len(base_model.names)} (COCO ë°ì´í„°ì…‹ ê¸°ë°˜)\")\n",
        "print(f\"ğŸ“‹ ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {len(custom_model.names)} (lane, traffic_sign)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: CHECK GPU AVAILABILITY AND MODEL OPTIMIZATION\n",
        "# ============================================================================\n",
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "cuda_available = torch.cuda.is_available()\n",
        "device_count = torch.cuda.device_count()\n",
        "\n",
        "print(f\"\\nğŸ” ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸:\")\n",
        "print(f\"   CUDA ì‚¬ìš© ê°€ëŠ¥: {cuda_available}\")\n",
        "print(f\"   GPU ê°œìˆ˜: {device_count}\")\n",
        "\n",
        "if cuda_available:\n",
        "    print(f\"   GPU ì¥ì¹˜: {torch.cuda.get_device_name(0)}\")\n",
        "    use_tensorrt = True\n",
        "    device = 0\n",
        "else:\n",
        "    print(\"   âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n",
        "    use_tensorrt = False\n",
        "    device = 'cpu'\n",
        "\n",
        "if use_tensorrt:\n",
        "    # ============================================================================\n",
        "    # STEP 2A: CONVERT MODELS TO TENSORRT FORMAT (GPU ONLY)\n",
        "    # ============================================================================\n",
        "    print(\"\\nâš¡ TensorRT ë³€í™˜ ì¤‘...\")\n",
        "    print(\"   TensorRTëŠ” NVIDIA GPUì—ì„œ ì¶”ë¡  ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¤ëŠ” ìµœì í™” ì—”ì§„ì…ë‹ˆë‹¤\")\n",
        "\n",
        "    try:\n",
        "        # Convert base model to TensorRT format with FP16 precision for speed optimization\n",
        "        print(\"ğŸ”„ ê¸°ë³¸ ëª¨ë¸ â†’ TensorRT ë³€í™˜ ì¤‘... (FP16 ì •ë°€ë„ë¡œ ìµœì í™”)\")\n",
        "        base_model.export(format='engine', half=True, device=device)\n",
        "        base_trt_path = 'yolo11n.engine'\n",
        "\n",
        "        # Convert custom model to TensorRT format\n",
        "        print(\"ğŸ”„ ì»¤ìŠ¤í…€ ëª¨ë¸ â†’ TensorRT ë³€í™˜ ì¤‘... (FP16 ì •ë°€ë„ë¡œ ìµœì í™”)\")\n",
        "        custom_model.export(format='engine', half=True, device=device)\n",
        "        custom_trt_path = '/content/dataset/best.engine'\n",
        "\n",
        "        # Load TensorRT optimized models\n",
        "        print(\"\\nğŸ”¥ TensorRT ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "        print(\"   ìµœì í™”ëœ ëª¨ë¸ì€ ì¼ë°˜ PyTorch ëª¨ë¸ë³´ë‹¤ 2-5ë°° ë¹ ë¥¸ ì¶”ë¡  ì†ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤\")\n",
        "\n",
        "        base_trt_model = YOLO(base_trt_path)      # TensorRT optimized base model\n",
        "        custom_trt_model = YOLO(custom_trt_path)  # TensorRT optimized custom model\n",
        "\n",
        "        print(\"âœ… TensorRT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ TensorRT ë³€í™˜ ì‹¤íŒ¨: {e}\")\n",
        "        print(\"ğŸ”„ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤...\")\n",
        "        use_tensorrt = False\n",
        "\n",
        "if not use_tensorrt:\n",
        "    # ============================================================================\n",
        "    # STEP 2B: USE PYTORCH MODELS ON CPU\n",
        "    # ============================================================================\n",
        "    print(\"\\nğŸ PyTorch CPU ëª¨ë“œë¡œ ì‹¤í–‰\")\n",
        "    print(\"   GPUê°€ ì—†ê±°ë‚˜ TensorRT ë³€í™˜ì— ì‹¤íŒ¨í•˜ì—¬ CPUì—ì„œ PyTorch ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤\")\n",
        "\n",
        "    # Use original PyTorch models\n",
        "    base_trt_model = base_model      # Use original base model\n",
        "    custom_trt_model = custom_model  # Use original custom model\n",
        "\n",
        "    print(\"âœ… PyTorch CPU ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: UPLOAD VIDEO FILES FROM PC\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ“¥ ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ\")\n",
        "print(\"   ë¸Œë¼ìš°ì € ë‹¤ì´ì–¼ë¡œê·¸ë¥¼ í†µí•´ PCì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”\")\n",
        "\n",
        "# Open file upload dialog for users to select video files from their PC\n",
        "print(\"\\nğŸ¬ ì—…ë¡œë“œí•  ë¹„ë””ì˜¤ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”:\")\n",
        "uploaded = files.upload()  # Returns a dictionary: {filename: file_content}\n",
        "\n",
        "# Check if any files were actually uploaded\n",
        "if not uploaded:\n",
        "    print(\"âŒ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "    raise Exception(\"No files uploaded\")  # Stop execution if no files uploaded\n",
        "\n",
        "print(f\"\\nâœ… {len(uploaded)} ê°œì˜ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤\")\n",
        "\n",
        "# Get the first uploaded video file path\n",
        "# The uploaded files are automatically saved to the current directory\n",
        "uploaded_filenames = list(uploaded.keys())\n",
        "video_path = uploaded_filenames[0]  # Use the first uploaded file\n",
        "print(f\"ğŸ“¹ ì²˜ë¦¬í•  ë¹„ë””ì˜¤: {video_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: PERFORMANCE COMPARISON FUNCTION\n",
        "# ============================================================================\n",
        "def performance_comparison(video_path, frames_to_test=100):\n",
        "    \"\"\"\n",
        "    Compare inference speed between standard and optimized models\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to the video file for testing\n",
        "        frames_to_test: Number of frames to use for speed comparison\n",
        "\n",
        "    Returns:\n",
        "        speedup_ratio: How many times faster optimized models are\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nâ±ï¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ (ì²« {frames_to_test}í”„ë ˆì„ìœ¼ë¡œ ì¸¡ì •)\")\n",
        "    if use_tensorrt:\n",
        "        print(\"   PyTorch vs TensorRT ì¶”ë¡  ì†ë„ë¥¼ ì •í™•íˆ ë¹„êµí•©ë‹ˆë‹¤\")\n",
        "    else:\n",
        "        print(\"   í‘œì¤€ PyTorch vs ìµœì í™”ëœ PyTorch ì¶”ë¡  ì†ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Open video capture object for reading frames\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # ========== Standard Models Performance Test ==========\n",
        "    print(\"ğŸ í‘œì¤€ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...\")\n",
        "    standard_times = []  # Store processing time for each frame\n",
        "\n",
        "    for i in range(frames_to_test):\n",
        "        ret, frame = cap.read()  # Read next frame\n",
        "        if not ret:  # End of video\n",
        "            break\n",
        "\n",
        "        # Measure inference time for both models\n",
        "        start_time = time.time()\n",
        "        _ = base_model(frame, verbose=False)      # Base model inference\n",
        "        _ = custom_model(frame, verbose=False)    # Custom model inference\n",
        "        end_time = time.time()\n",
        "\n",
        "        standard_times.append(end_time - start_time)\n",
        "\n",
        "    # ========== Optimized Models Performance Test ==========\n",
        "    if use_tensorrt:\n",
        "        print(\"âš¡ TensorRT ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...\")\n",
        "    else:\n",
        "        print(\"ğŸ”§ ìµœì í™”ëœ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...\")\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset video to beginning\n",
        "    optimized_times = []  # Store processing time for each frame\n",
        "\n",
        "    for i in range(frames_to_test):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Measure inference time for both optimized models\n",
        "        start_time = time.time()\n",
        "        _ = base_trt_model(frame, verbose=False)     # Optimized base model\n",
        "        _ = custom_trt_model(frame, verbose=False)   # Optimized custom model\n",
        "        end_time = time.time()\n",
        "\n",
        "        optimized_times.append(end_time - start_time)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # ========== Calculate and Display Results ==========\n",
        "    standard_avg = np.mean(standard_times) * 1000    # Convert to milliseconds\n",
        "    optimized_avg = np.mean(optimized_times) * 1000\n",
        "    speedup = standard_avg / optimized_avg           # Calculate speedup ratio\n",
        "\n",
        "    model_type = \"TensorRT\" if use_tensorrt else \"ìµœì í™”ëœ PyTorch\"\n",
        "\n",
        "    print(f\"ğŸ í‘œì¤€ PyTorch í‰ê· : {standard_avg:.2f}ms/frame ({1000/standard_avg:.1f} FPS)\")\n",
        "    print(f\"âš¡ {model_type} í‰ê· : {optimized_avg:.2f}ms/frame ({1000/optimized_avg:.1f} FPS)\")\n",
        "    print(f\"ğŸš€ ì†ë„ í–¥ìƒ: {speedup:.2f}x ë¹¨ë¼ì§\")\n",
        "\n",
        "    return speedup\n",
        "\n",
        "# Execute performance comparison\n",
        "speedup_ratio = performance_comparison(video_path)\n",
        "\n",
        "# Set model type for display purposes\n",
        "model_type = \"TensorRT\" if use_tensorrt else \"ìµœì í™”ëœ\"\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: OPTIMIZED COMBINED INFERENCE\n",
        "# ============================================================================\n",
        "def optimized_combined_inference(video_path, output_path='/content/optimized_result.mp4'):\n",
        "    \"\"\"\n",
        "    Process entire video using optimized models with visual output\n",
        "\n",
        "    Args:\n",
        "        video_path: Input video file path\n",
        "        output_path: Output processed video file path\n",
        "\n",
        "    Returns:\n",
        "        avg_fps: Average processing speed in frames per second\n",
        "    \"\"\"\n",
        "\n",
        "    # ========== Video Properties Setup ==========\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))           # Original video FPS\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Video width\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Video height\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Total frame count\n",
        "\n",
        "    # Setup output video writer with same properties as input\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Video codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    model_type = \"TensorRT\" if use_tensorrt else \"ìµœì í™”ëœ\"\n",
        "    print(f\"\\nğŸ¬ {model_type} ì˜ìƒ ì²˜ë¦¬ ì‹œì‘...\")\n",
        "    print(f\"   ğŸ“¹ í•´ìƒë„: {width}x{height}\")\n",
        "    print(f\"   ğŸï¸ FPS: {fps}\")\n",
        "    print(f\"   ğŸ“Š ì´ í”„ë ˆì„: {total_frames}\")\n",
        "    print(f\"   ğŸ–¥ï¸ ì²˜ë¦¬ ì¥ì¹˜: {'GPU (TensorRT)' if use_tensorrt else 'CPU (PyTorch)'}\")\n",
        "\n",
        "    # ========== Frame-by-Frame Processing ==========\n",
        "    frame_count = 0\n",
        "    total_inference_time = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:  # End of video\n",
        "            break\n",
        "\n",
        "        # ========== Optimized Inference (with timing) ==========\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run inference on both optimized models\n",
        "        base_results = base_trt_model(frame, verbose=False)     # General objects\n",
        "        custom_results = custom_trt_model(frame, verbose=False) # Lanes & traffic signs\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "        total_inference_time += inference_time\n",
        "\n",
        "        # ========== Visualization of Detection Results ==========\n",
        "        annotated_frame = frame.copy()\n",
        "\n",
        "        # Draw base YOLO detection results (BLUE boxes)\n",
        "        if base_results[0].boxes is not None:\n",
        "            for box in base_results[0].boxes:\n",
        "                # Extract bounding box coordinates\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])    # Confidence score\n",
        "                cls = int(box.cls[0])        # Class index\n",
        "\n",
        "                # Only draw boxes with confidence > 0.3\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{base_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    # Draw blue rectangle and text\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # Draw custom YOLO detection results (RED boxes)\n",
        "        if custom_results[0].boxes is not None:\n",
        "            for box in custom_results[0].boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{custom_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    # Draw red rectangle and text\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # Add real-time FPS information (GREEN text)\n",
        "        fps_text = f\"{model_type}: {1/inference_time:.1f} FPS\"\n",
        "        cv2.putText(annotated_frame, fps_text, (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Write processed frame to output video\n",
        "        out.write(annotated_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "        # Progress reporting every 50 frames\n",
        "        if frame_count % 50 == 0:\n",
        "            avg_fps = frame_count / total_inference_time\n",
        "            progress = frame_count / total_frames * 100\n",
        "            print(f\"   ğŸ”„ ì²˜ë¦¬ ì¤‘... {frame_count}/{total_frames} ({progress:.1f}%) - í‰ê·  {avg_fps:.1f} FPS\")\n",
        "\n",
        "    # ========== Cleanup and Results ==========\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    avg_fps = frame_count / total_inference_time\n",
        "    print(f\"âœ… {model_type} ê²°ê³¼ ì˜ìƒ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
        "    print(f\"ğŸ“Š ìµœì¢… í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_fps:.1f} FPS\")\n",
        "\n",
        "    return avg_fps\n",
        "    \"\"\"\n",
        "    Process entire video using TensorRT optimized models with visual output\n",
        "\n",
        "    Args:\n",
        "        video_path: Input video file path\n",
        "        output_path: Output processed video file path\n",
        "\n",
        "    Returns:\n",
        "        avg_fps: Average processing speed in frames per second\n",
        "    \"\"\"\n",
        "\n",
        "    # ========== Video Properties Setup ==========\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))           # Original video FPS\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Video width\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Video height\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Total frame count\n",
        "\n",
        "    # Setup output video writer with same properties as input\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Video codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    print(f\"\\nğŸ¬ TensorRT ìµœì í™” ì˜ìƒ ì²˜ë¦¬ ì‹œì‘...\")\n",
        "    print(f\"   ğŸ“¹ í•´ìƒë„: {width}x{height}\")\n",
        "    print(f\"   ğŸï¸ FPS: {fps}\")\n",
        "    print(f\"   ğŸ“Š ì´ í”„ë ˆì„: {total_frames}\")\n",
        "\n",
        "    # ========== Frame-by-Frame Processing ==========\n",
        "    frame_count = 0\n",
        "    total_inference_time = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:  # End of video\n",
        "            break\n",
        "\n",
        "        # ========== TensorRT Inference (with timing) ==========\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run inference on both TensorRT optimized models\n",
        "        base_results = base_trt_model(frame, verbose=False)     # General objects\n",
        "        custom_results = custom_trt_model(frame, verbose=False) # Lanes & traffic signs\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "        total_inference_time += inference_time\n",
        "\n",
        "        # ========== Visualization of Detection Results ==========\n",
        "        annotated_frame = frame.copy()\n",
        "\n",
        "        # Draw base YOLO detection results (BLUE boxes)\n",
        "        if base_results[0].boxes is not None:\n",
        "            for box in base_results[0].boxes:\n",
        "                # Extract bounding box coordinates\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])    # Confidence score\n",
        "                cls = int(box.cls[0])        # Class index\n",
        "\n",
        "                # Only draw boxes with confidence > 0.3\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{base_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    # Draw blue rectangle and text\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # Draw custom YOLO detection results (RED boxes)\n",
        "        if custom_results[0].boxes is not None:\n",
        "            for box in custom_results[0].boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{custom_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    # Draw red rectangle and text\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # Add real-time FPS information (GREEN text)\n",
        "        fps_text = f\"TensorRT: {1/inference_time:.1f} FPS\"\n",
        "        cv2.putText(annotated_frame, fps_text, (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Write processed frame to output video\n",
        "        out.write(annotated_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "        # Progress reporting every 50 frames\n",
        "        if frame_count % 50 == 0:\n",
        "            avg_fps = frame_count / total_inference_time\n",
        "            progress = frame_count / total_frames * 100\n",
        "            print(f\"   ğŸ”„ ì²˜ë¦¬ ì¤‘... {frame_count}/{total_frames} ({progress:.1f}%) - í‰ê·  {avg_fps:.1f} FPS\")\n",
        "\n",
        "    # ========== Cleanup and Results ==========\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    avg_fps = frame_count / total_inference_time\n",
        "    print(f\"âœ… TensorRT ê²°ê³¼ ì˜ìƒ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
        "    print(f\"ğŸ“Š ìµœì¢… í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_fps:.1f} FPS\")\n",
        "\n",
        "    return avg_fps\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: EXECUTE OPTIMIZED INFERENCE\n",
        "# ============================================================================\n",
        "print(f\"\\nğŸ”¥ {model_type} ìµœì í™”ëœ ê²°í•© ì¶”ë¡  ì‹¤í–‰...\")\n",
        "if use_tensorrt:\n",
        "    print(\"   ì „ì²´ ë¹„ë””ì˜¤ë¥¼ TensorRTë¡œ ìµœì í™”ëœ ë‘ ëª¨ë¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤\")\n",
        "    output_filename = '/content/tensorrt_final_result.mp4'\n",
        "    tensorrt_fps = optimized_combined_inference(video_path, output_filename) # Assign returned value\n",
        "else:\n",
        "    print(\"   ì „ì²´ ë¹„ë””ì˜¤ë¥¼ CPUì—ì„œ ìµœì í™”ëœ ë‘ ëª¨ë¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤\")\n",
        "    output_filename = '/content/cpu_final_result.mp4'\n",
        "    # When not using TensorRT, optimized_combined_inference uses the \"optimized\" PyTorch models.\n",
        "    # We will assign its result to optimized_fps as it represents the performance of the optimized path.\n",
        "    optimized_fps = optimized_combined_inference(video_path, output_filename)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: STANDARD PYTORCH INFERENCE FOR COMPARISON\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ í‘œì¤€ PyTorch ì¶”ë¡  (ë¹„êµ ê¸°ì¤€ìš©)...\")\n",
        "print(\"   ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•´ ë™ì¼í•œ ì˜ìƒì„ í‘œì¤€ PyTorch ëª¨ë¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤\")\n",
        "\n",
        "def standard_pytorch_inference(video_path, output_path='/content/standard_pytorch_result.mp4'):\n",
        "    \"\"\"\n",
        "    Process video using standard PyTorch models for performance comparison\n",
        "\n",
        "    Args:\n",
        "        video_path: Input video file path\n",
        "        output_path: Output video file path\n",
        "\n",
        "    Returns:\n",
        "        standard_fps: Average processing speed in FPS\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Setup video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Run inference with standard PyTorch models\n",
        "        base_results = base_model(frame, verbose=False)\n",
        "        custom_results = custom_model(frame, verbose=False)\n",
        "\n",
        "        # Simple annotation for comparison (without detailed boxes)\n",
        "        annotated_frame = frame.copy()\n",
        "        cv2.putText(annotated_frame, \"Standard PyTorch\", (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
        "\n",
        "        out.write(annotated_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "        # Process only first 100 frames for quick comparison\n",
        "        if frame_count >= 100:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    standard_fps = frame_count / total_time\n",
        "    return standard_fps\n",
        "\n",
        "# Execute standard PyTorch inference for comparison\n",
        "# Assign the returned value to pytorch_fps\n",
        "pytorch_fps = standard_pytorch_inference(video_path, output_path='/content/pytorch_result.mp4')\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: MODEL PERFORMANCE EVALUATION\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ“Š ì»¤ìŠ¤í…€ ëª¨ë¸ ì •í™•ë„ í‰ê°€:\")\n",
        "print(\"   Validation ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤\")\n",
        "\n",
        "# Evaluate custom model performance on validation dataset\n",
        "metrics = custom_model.val(data='/content/dataset/dataset_fixed.yaml')\n",
        "print(f\"ğŸ¯ mAP50 (Mean Average Precision): {metrics.box.map50:.4f}\")\n",
        "print(\"   mAP50ì€ IoU 0.5ì—ì„œì˜ í‰ê·  ì •ë°€ë„ë¡œ, ë†’ì„ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤ (ìµœëŒ€ê°’: 1.0)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: FINAL RESULTS AND COMPARISON\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ¯ ìµœì¢… ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:\")\n",
        "print(\"-\" * 30)\n",
        "# Use the correct variables based on whether TensorRT was used\n",
        "if use_tensorrt:\n",
        "    print(f\"ğŸ PyTorch ì²˜ë¦¬ ì†ë„:  {pytorch_fps:.1f} FPS\")\n",
        "    print(f\"âš¡ TensorRT ì²˜ë¦¬ ì†ë„:  {tensorrt_fps:.1f} FPS\")\n",
        "    print(f\"ğŸš€ ì „ì²´ ì†ë„ í–¥ìƒ ë¹„ìœ¨: {tensorrt_fps/pytorch_fps:.2f}x ë¹¨ë¼ì§\")\n",
        "else:\n",
        "    # If TensorRT was not used, compare standard PyTorch with the \"optimized\" PyTorch run\n",
        "    print(f\"ğŸ í‘œì¤€ PyTorch ì²˜ë¦¬ ì†ë„:  {pytorch_fps:.1f} FPS\")\n",
        "    print(f\"ğŸ”§ ìµœì í™”ëœ PyTorch ì²˜ë¦¬ ì†ë„:  {optimized_fps:.1f} FPS\")\n",
        "    print(f\"ğŸš€ ì „ì²´ ì†ë„ í–¥ìƒ ë¹„ìœ¨: {optimized_fps/pytorch_fps:.2f}x ë¹¨ë¼ì§\")\n",
        "\n",
        "\n",
        "print(f\"ğŸ“Š ëª¨ë¸ ì •í™•ë„ (mAP50): {metrics.box.map50:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ¬ ìµœì¢… TensorRT ìµœì í™” ê²°ê³¼ ì˜ìƒ:\")\n",
        "# Display the correct output video based on whether TensorRT was used\n",
        "if use_tensorrt:\n",
        "    Video('/content/tensorrt_final_result.mp4', width=800)\n",
        "else:\n",
        "    Video('/content/cpu_final_result.mp4', width=800)\n",
        "\n",
        "\n",
        "print(\"\\nğŸ‰ ìµœì í™” ì™„ë£Œ!\")\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“‹ ê²°ê³¼ í•´ì„ ê°€ì´ë“œ:\")\n",
        "print(\"ğŸ”µ íŒŒë€ìƒ‰ ë°•ìŠ¤: ê¸°ë³¸ YOLOê°€ íƒì§€í•œ ì¼ë°˜ ê°ì²´ë“¤ (TensorRT ìµœì í™”)\")\n",
        "print(\"ğŸ”´ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤: ì»¤ìŠ¤í…€ ëª¨ë¸ì´ íƒì§€í•œ ì°¨ì„ /êµí†µí‘œì§€íŒ (TensorRT ìµœì í™”)\")\n",
        "print(\"ğŸ’š ì´ˆë¡ìƒ‰ í…ìŠ¤íŠ¸: ì‹¤ì‹œê°„ FPS í‘œì‹œ (ì²˜ë¦¬ ì†ë„ ëª¨ë‹ˆí„°ë§)\")\n",
        "\n",
        "print(\"\\nğŸ’¾ ìƒì„±ëœ íŒŒì¼ë“¤:\")\n",
        "if use_tensorrt:\n",
        "    print(\"- tensorrt_final_result.mp4: TensorRT ìµœì í™”ëœ ìµœì¢… ê²°ê³¼ ì˜ìƒ\")\n",
        "    print(\"- yolo11n.engine: ê¸°ë³¸ YOLO ëª¨ë¸ì˜ TensorRT ì—”ì§„ íŒŒì¼\")\n",
        "    print(\"- best.engine: ì»¤ìŠ¤í…€ ëª¨ë¸ì˜ TensorRT ì—”ì§„ íŒŒì¼\")\n",
        "else:\n",
        "    print(\"- cpu_final_result.mp4: CPUì—ì„œ ìµœì í™”ëœ ìµœì¢… ê²°ê³¼ ì˜ìƒ\")\n",
        "print(\"- pytorch_result.mp4: PyTorch ê¸°ë³¸ ëª¨ë¸ ë¹„êµìš© ì˜ìƒ\")\n",
        "\n",
        "\n",
        "print(\"\\nğŸš€ ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ!\")\n",
        "# Use the correct variables based on whether TensorRT was used\n",
        "if use_tensorrt:\n",
        "    print(f\"   ìµœì¢… ì†ë„ í–¥ìƒ: {speedup_ratio:.1f}x (ê°œë³„ í”„ë ˆì„ ê¸°ì¤€)\")\n",
        "    print(f\"   ì „ì²´ ì˜ìƒ ì²˜ë¦¬: {tensorrt_fps/pytorch_fps:.1f}x (ì „ì²´ ì˜ìƒ ê¸°ì¤€)\")\n",
        "else:\n",
        "    print(f\"   ìµœì¢… ì†ë„ í–¥ìƒ: {speedup_ratio:.1f}x (ê°œë³„ í”„ë ˆì„ ê¸°ì¤€)\")\n",
        "    print(f\"   ì „ì²´ ì˜ìƒ ì²˜ë¦¬: {optimized_fps/pytorch_fps:.1f}x (ì „ì²´ ì˜ìƒ ê¸°ì¤€)\")"
      ],
      "metadata": {
        "id": "JF5702JNN-Hv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}